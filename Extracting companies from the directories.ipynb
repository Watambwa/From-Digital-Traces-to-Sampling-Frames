{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extracting companies from each directories",
   "id": "76fc00646abcf9b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Web Crawling on directories",
   "id": "aa1d42963910236a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-03T12:50:05.344715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "============================================\n",
    "Enhanced Zimbabwe Company Directory Scraper\n",
    "============================================\n",
    "Features:\n",
    "- Breadth-First Search (BFS) algorithm\n",
    "- Progressive checkpoint saving\n",
    "- Performance statistics & visualization\n",
    "- Network graph visualization\n",
    "- City and description extraction\n",
    "- Internet interruption resilient\n",
    "- Publication-ready analytics\n",
    "\n",
    "Author: Enhanced Version\n",
    "Date: 2025\n",
    "============================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Set, Dict, List, Optional, Tuple\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    # User agents for rotation\n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 Version/17.0 Safari/605.1.15\",\n",
    "    ]\n",
    "\n",
    "    # Request settings\n",
    "    REQUEST_TIMEOUT = 15\n",
    "    SLEEP_MIN, SLEEP_MAX = 0.3, 0.8  # Faster but respectful\n",
    "    RETRY_ATTEMPTS = 3\n",
    "\n",
    "    # Crawl limits (preventing infinite loops)\n",
    "    MAX_PAGES_PER_SEED = 3800          # Maximum pages per initial URL\n",
    "    MAX_DEPTH = 1                     # Maximum BFS depth\n",
    "    MAX_DETAIL_PAGES = 2000            # Maximum detail pages per seed\n",
    "    MAX_CATEGORIES = 800              # Maximum categories to explore\n",
    "\n",
    "    # ZimbabweYP specific settings\n",
    "    ZYP_MAX_PAGES_PER_CATEGORY = 100  # Reduced for speed\n",
    "    ZYP_DETAIL_WORKERS = 6            # Parallel detail fetching\n",
    "\n",
    "    # Checkpoint settings\n",
    "    CHECKPOINT_INTERVAL = 50          # Save every N companies\n",
    "    CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    STATS_DIR = Path(\"statistics\")\n",
    "\n",
    "    # Country\n",
    "    COUNTRY = \"Zimbabwe\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [Config.CHECKPOINT_DIR, Config.OUTPUT_DIR, Config.STATS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# DATA STRUCTURES\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class Company:\n",
    "    \"\"\"Company data structure\"\"\"\n",
    "    name: str = \"\"\n",
    "    address: str = \"\"\n",
    "    city: str = \"\"\n",
    "    phone: str = \"\"\n",
    "    email: str = \"\"\n",
    "    website: str = \"\"\n",
    "    description: str = \"\"\n",
    "    source_url: str = \"\"\n",
    "    country: str = Config.COUNTRY\n",
    "    crawl_timestamp: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class CrawlStats:\n",
    "    \"\"\"Statistics for a crawl session\"\"\"\n",
    "    seed_url: str\n",
    "    start_time: float\n",
    "    end_time: float = 0\n",
    "    pages_crawled: int = 0\n",
    "    detail_pages_crawled: int = 0\n",
    "    companies_found: int = 0\n",
    "    errors: int = 0\n",
    "    avg_response_time: float = 0\n",
    "    success_rate: float = 0\n",
    "\n",
    "    def duration(self) -> float:\n",
    "        return self.end_time - self.start_time if self.end_time else time.time() - self.start_time\n",
    "\n",
    "    def pages_per_minute(self) -> float:\n",
    "        duration_min = self.duration() / 60\n",
    "        return self.pages_crawled / duration_min if duration_min > 0 else 0\n",
    "\n",
    "# ============================================\n",
    "# UTILITIES\n",
    "# ============================================\n",
    "\n",
    "class SessionManager:\n",
    "    \"\"\"Manages HTTP sessions with retry logic\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_session() -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=Config.RETRY_ATTEMPTS,\n",
    "            backoff_factor=0.3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=20)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    @staticmethod\n",
    "    def get_headers() -> Dict[str, str]:\n",
    "        return {\n",
    "            \"User-Agent\": random.choice(Config.USER_AGENTS),\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "        }\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Text cleaning and extraction utilities\"\"\"\n",
    "\n",
    "    PHONE_RE = re.compile(r\"(?:\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d)\")\n",
    "    EMAIL_RE = re.compile(r\"[A-Z0-9._%+\\-]+@[A-Z0-9.\\-]+\\.[A-Z]{2,}\", re.I)\n",
    "\n",
    "    # Zimbabwe city patterns\n",
    "    CITIES = [\n",
    "        \"Harare\", \"Bulawayo\", \"Chitungwiza\", \"Mutare\", \"Gweru\", \"Kwekwe\",\n",
    "        \"Kadoma\", \"Masvingo\", \"Chinhoyi\", \"Marondera\", \"Norton\", \"Chegutu\",\n",
    "        \"Bindura\", \"Beitbridge\", \"Redcliff\", \"Victoria Falls\", \"Hwange\",\n",
    "        \"Rusape\", \"Chiredzi\", \"Kariba\", \"Karoi\", \"Zvishavane\"\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def clean(text) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_phones(text: str) -> List[str]:\n",
    "        return list({TextProcessor.clean(m.group(0)) for m in TextProcessor.PHONE_RE.finditer(text or \"\")})\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emails(text: str) -> List[str]:\n",
    "        return list({TextProcessor.clean(m.group(0)) for m in TextProcessor.EMAIL_RE.finditer(text or \"\")})\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_city(text: str) -> str:\n",
    "        \"\"\"Extract city name from address/text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text_upper = text.upper()\n",
    "        for city in TextProcessor.CITIES:\n",
    "            if city.upper() in text_upper:\n",
    "                return city\n",
    "        return \"\"\n",
    "\n",
    "class URLUtils:\n",
    "    \"\"\"URL manipulation utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_domain(url: str) -> str:\n",
    "        try:\n",
    "            return urlparse(url).netloc.lower().replace(\"www.\", \"\")\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def same_domain(url1: str, url2: str) -> bool:\n",
    "        return URLUtils.get_domain(url1) == URLUtils.get_domain(url2)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(url: str) -> str:\n",
    "        try:\n",
    "            p = urlparse(url)\n",
    "            return f\"{p.scheme}://{p.netloc}{p.path or '/'}\"\n",
    "        except:\n",
    "            return url\n",
    "\n",
    "    @staticmethod\n",
    "    def is_zimbabwe_yp(url: str) -> bool:\n",
    "        return \"zimbabweyp.com\" in URLUtils.get_domain(url)\n",
    "\n",
    "def sleep_random():\n",
    "    \"\"\"Random sleep to avoid rate limiting\"\"\"\n",
    "    time.sleep(random.uniform(Config.SLEEP_MIN, Config.SLEEP_MAX))\n",
    "\n",
    "# ============================================\n",
    "# HTTP & PARSING\n",
    "# ============================================\n",
    "\n",
    "class HTTPClient:\n",
    "    \"\"\"HTTP client with timing and error tracking\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = SessionManager.create_session()\n",
    "        self.response_times = []\n",
    "        self.errors = 0\n",
    "        self.success = 0\n",
    "\n",
    "    def get_soup(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Fetch URL and return BeautifulSoup object\"\"\"\n",
    "        start = time.time()\n",
    "        try:\n",
    "            resp = self.session.get(\n",
    "                url,\n",
    "                headers=SessionManager.get_headers(),\n",
    "                timeout=Config.REQUEST_TIMEOUT,\n",
    "                allow_redirects=True\n",
    "            )\n",
    "\n",
    "            if resp.status_code >= 400:\n",
    "                self.errors += 1\n",
    "                return None\n",
    "\n",
    "            if not resp.encoding:\n",
    "                resp.encoding = \"utf-8\"\n",
    "\n",
    "            self.response_times.append(time.time() - start)\n",
    "            self.success += 1\n",
    "            return BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.errors += 1\n",
    "            return None\n",
    "\n",
    "    def avg_response_time(self) -> float:\n",
    "        return sum(self.response_times) / len(self.response_times) if self.response_times else 0\n",
    "\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.success + self.errors\n",
    "        return (self.success / total * 100) if total > 0 else 0\n",
    "\n",
    "# ============================================\n",
    "# EXTRACTION LOGIC\n",
    "# ============================================\n",
    "\n",
    "class CompanyExtractor:\n",
    "    \"\"\"Extract company information from pages\"\"\"\n",
    "\n",
    "    ORG_TYPES = {\"Organization\", \"LocalBusiness\", \"Corporation\", \"Company\", \"NGO\", \"Store\"}\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json_ld(soup: BeautifulSoup, source_url: str) -> List[Company]:\n",
    "        \"\"\"Extract companies from JSON-LD structured data\"\"\"\n",
    "        companies = []\n",
    "\n",
    "        for tag in soup.find_all(\"script\", type=lambda t: t and \"ld+json\" in t):\n",
    "            try:\n",
    "                data = json.loads(tag.string or \"\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            items = []\n",
    "            if isinstance(data, list):\n",
    "                items = data\n",
    "            elif isinstance(data, dict):\n",
    "                if \"@graph\" in data:\n",
    "                    items = data[\"@graph\"]\n",
    "                else:\n",
    "                    items = [data]\n",
    "\n",
    "            for item in items:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "\n",
    "                item_type = item.get(\"@type\", \"\")\n",
    "                types = {item_type} if isinstance(item_type, str) else set(item_type or [])\n",
    "\n",
    "                if not types.intersection(CompanyExtractor.ORG_TYPES):\n",
    "                    continue\n",
    "\n",
    "                name = TextProcessor.clean(item.get(\"name\", \"\"))\n",
    "                if not name:\n",
    "                    continue\n",
    "\n",
    "                # Parse address\n",
    "                addr = item.get(\"address\", \"\")\n",
    "                if isinstance(addr, dict):\n",
    "                    addr_parts = [\n",
    "                        addr.get(\"streetAddress\", \"\"),\n",
    "                        addr.get(\"addressLocality\", \"\"),\n",
    "                        addr.get(\"addressRegion\", \"\"),\n",
    "                        addr.get(\"postalCode\", \"\"),\n",
    "                    ]\n",
    "                    address = TextProcessor.clean(\" \".join(p for p in addr_parts if p))\n",
    "                else:\n",
    "                    address = TextProcessor.clean(addr)\n",
    "\n",
    "                company = Company(\n",
    "                    name=name,\n",
    "                    address=address,\n",
    "                    city=TextProcessor.extract_city(address),\n",
    "                    phone=TextProcessor.clean(item.get(\"telephone\", \"\")),\n",
    "                    email=TextProcessor.clean(item.get(\"email\", \"\")),\n",
    "                    website=TextProcessor.clean(item.get(\"url\", \"\")),\n",
    "                    description=TextProcessor.clean(item.get(\"description\", \"\")),\n",
    "                    source_url=source_url,\n",
    "                    crawl_timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "                companies.append(company)\n",
    "\n",
    "        return companies\n",
    "\n",
    "    @staticmethod\n",
    "    def from_listing_blocks(soup: BeautifulSoup, base_url: str) -> Tuple[List[Company], List[str]]:\n",
    "        \"\"\"Extract companies and detail links from listing blocks\"\"\"\n",
    "        companies = []\n",
    "        detail_links = set()\n",
    "\n",
    "        # Common listing selectors\n",
    "        selectors = [\n",
    "            'div[class*=\"listing\"]', 'div[class*=\"result\"]', 'div[class*=\"company\"]',\n",
    "            'div[class*=\"business\"]', 'li[class*=\"listing\"]', 'article', '.card'\n",
    "        ]\n",
    "\n",
    "        blocks = []\n",
    "        for sel in selectors:\n",
    "            blocks.extend(soup.select(sel))\n",
    "\n",
    "        for block in blocks[:100]:  # Limit to avoid over-processing\n",
    "            try:\n",
    "                # Extract name\n",
    "                name_elem = (\n",
    "                    block.select_one('h2 a, h3 a, h4 a, .name a, .title a') or\n",
    "                    block.select_one('h2, h3, h4, .name, .title, strong')\n",
    "                )\n",
    "                name = TextProcessor.clean(name_elem.get_text()) if name_elem else \"\"\n",
    "\n",
    "                if not name:\n",
    "                    continue\n",
    "\n",
    "                # Extract address\n",
    "                addr_elem = block.select_one('address, .address, .location, [itemprop=\"address\"]')\n",
    "                address = TextProcessor.clean(addr_elem.get_text()) if addr_elem else \"\"\n",
    "\n",
    "                # Extract contact info\n",
    "                phone_elem = block.select_one('a[href^=\"tel:\"], .phone, [itemprop=\"telephone\"]')\n",
    "                phone = TextProcessor.clean(phone_elem.get_text()) if phone_elem else \"\"\n",
    "\n",
    "                email_elem = block.select_one('a[href^=\"mailto:\"], .email')\n",
    "                email = TextProcessor.clean(email_elem.get_text()) if email_elem else \"\"\n",
    "\n",
    "                # Extract description\n",
    "                desc_elem = block.select_one('.description, .about, .summary, p')\n",
    "                description = TextProcessor.clean(desc_elem.get_text()[:500]) if desc_elem else \"\"\n",
    "\n",
    "                # Find links\n",
    "                website = \"\"\n",
    "                detail_url = None\n",
    "\n",
    "                for a in block.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if href.startswith((\"mailto:\", \"tel:\", \"javascript:\", \"#\")):\n",
    "                        continue\n",
    "\n",
    "                    full_url = urljoin(base_url, href)\n",
    "\n",
    "                    if not URLUtils.same_domain(full_url, base_url):\n",
    "                        website = full_url\n",
    "                    elif not detail_url:\n",
    "                        detail_url = full_url\n",
    "\n",
    "                # Fallback: extract from block text\n",
    "                block_text = block.get_text()\n",
    "                if not phone:\n",
    "                    phones = TextProcessor.extract_phones(block_text)\n",
    "                    phone = phones[0] if phones else \"\"\n",
    "                if not email:\n",
    "                    emails = TextProcessor.extract_emails(block_text)\n",
    "                    email = emails[0] if emails else \"\"\n",
    "\n",
    "                company = Company(\n",
    "                    name=name,\n",
    "                    address=address,\n",
    "                    city=TextProcessor.extract_city(address),\n",
    "                    phone=phone,\n",
    "                    email=email,\n",
    "                    website=website,\n",
    "                    description=description,\n",
    "                    source_url=detail_url or base_url,\n",
    "                    crawl_timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "                companies.append(company)\n",
    "\n",
    "                if detail_url:\n",
    "                    detail_links.add(detail_url)\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        return companies, list(detail_links)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_detail_page(soup: BeautifulSoup, url: str) -> Optional[Company]:\n",
    "        \"\"\"Extract company from detail page\"\"\"\n",
    "        # Try JSON-LD first\n",
    "        jsonld_companies = CompanyExtractor.from_json_ld(soup, url)\n",
    "        if jsonld_companies:\n",
    "            return jsonld_companies[0]\n",
    "\n",
    "        # Heuristic extraction\n",
    "        name_elem = soup.select_one('h1, .company-name, .page-title, [itemprop=\"name\"]')\n",
    "        name = TextProcessor.clean(name_elem.get_text()) if name_elem else \"\"\n",
    "\n",
    "        if not name:\n",
    "            title = soup.find(\"title\")\n",
    "            name = TextProcessor.clean(title.get_text()) if title else \"\"\n",
    "\n",
    "        addr_elem = soup.select_one('address, .address, .location, [itemprop=\"address\"]')\n",
    "        address = TextProcessor.clean(addr_elem.get_text()) if addr_elem else \"\"\n",
    "\n",
    "        # Extract description/about\n",
    "        desc_elem = soup.select_one('.about, .description, .company-info, [itemprop=\"description\"]')\n",
    "        description = \"\"\n",
    "        if desc_elem:\n",
    "            description = TextProcessor.clean(desc_elem.get_text()[:1000])\n",
    "\n",
    "        # Extract from full text\n",
    "        page_text = soup.get_text()\n",
    "        phones = TextProcessor.extract_phones(page_text)\n",
    "        emails = TextProcessor.extract_emails(page_text)\n",
    "\n",
    "        # Find website\n",
    "        website = \"\"\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            text = a.get_text().lower()\n",
    "            if \"website\" in text or \"visit\" in text:\n",
    "                website = urljoin(url, a[\"href\"])\n",
    "                break\n",
    "\n",
    "        return Company(\n",
    "            name=name,\n",
    "            address=address,\n",
    "            city=TextProcessor.extract_city(address),\n",
    "            phone=phones[0] if phones else \"\",\n",
    "            email=emails[0] if emails else \"\",\n",
    "            website=website,\n",
    "            description=description,\n",
    "            source_url=url,\n",
    "            crawl_timestamp=datetime.now().isoformat()\n",
    "        ) if name else None\n",
    "\n",
    "# ============================================\n",
    "# PAGINATION\n",
    "# ============================================\n",
    "\n",
    "class PaginationHandler:\n",
    "    \"\"\"Handle pagination across different sites\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def find_next_page(soup: BeautifulSoup, current_url: str) -> Optional[str]:\n",
    "        \"\"\"Find the next page URL\"\"\"\n",
    "        # Check rel=\"next\"\n",
    "        for a in soup.find_all(\"a\", href=True, rel=True):\n",
    "            rels = a.get(\"rel\")\n",
    "            if isinstance(rels, str):\n",
    "                rels = [rels]\n",
    "            if any(\"next\" in r.lower() for r in (rels or [])):\n",
    "                return urljoin(current_url, a[\"href\"])\n",
    "\n",
    "        # Check text/aria labels\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            text = a.get_text().lower()\n",
    "            classes = \" \".join(a.get(\"class\", [])).lower()\n",
    "            aria = (a.get(\"aria-label\") or \"\").lower()\n",
    "\n",
    "            if any(term in text for term in [\"next\", \"¬ª\", \">\"]) or \"next\" in classes or \"next\" in aria:\n",
    "                return urljoin(current_url, a[\"href\"])\n",
    "\n",
    "        # Check URL patterns\n",
    "        # Pattern: ?page=N\n",
    "        match = re.search(r\"([?&])(page|p)=(\\d+)\", current_url, re.I)\n",
    "        if match:\n",
    "            next_num = int(match.group(3)) + 1\n",
    "            return re.sub(r\"([?&])(page|p)=\\d+\", rf\"\\1\\2={next_num}\", current_url)\n",
    "\n",
    "        # Pattern: /page/N/\n",
    "        match = re.search(r\"/page/(\\d+)/?$\", current_url, re.I)\n",
    "        if match:\n",
    "            next_num = int(match.group(1)) + 1\n",
    "            return re.sub(r\"/page/\\d+/?$\", f\"/page/{next_num}/\", current_url)\n",
    "\n",
    "        return None\n",
    "\n",
    "# ============================================\n",
    "# BFS CRAWLER\n",
    "# ============================================\n",
    "\n",
    "class BFSCrawler:\n",
    "    \"\"\"Breadth-First Search web crawler with checkpointing\"\"\"\n",
    "\n",
    "    def __init__(self, seed_url: str, checkpoint_id: str):\n",
    "        self.seed_url = seed_url\n",
    "        self.checkpoint_id = checkpoint_id\n",
    "        self.domain = URLUtils.get_domain(seed_url)\n",
    "\n",
    "        # BFS data structures\n",
    "        self.queue = deque([(seed_url, 0)])  # (url, depth)\n",
    "        self.visited_listing = set()\n",
    "        self.visited_detail = set()\n",
    "\n",
    "        # Results\n",
    "        self.companies: List[Company] = []\n",
    "        self.stats = CrawlStats(seed_url=seed_url, start_time=time.time())\n",
    "\n",
    "        # Network graph for visualization\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.graph.add_node(seed_url, type=\"seed\", depth=0)\n",
    "\n",
    "        # HTTP client\n",
    "        self.client = HTTPClient()\n",
    "\n",
    "        # Load checkpoint if exists\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        \"\"\"Load progress from checkpoint\"\"\"\n",
    "        checkpoint_file = Config.CHECKPOINT_DIR / f\"{self.checkpoint_id}.pkl\"\n",
    "        if checkpoint_file.exists():\n",
    "            try:\n",
    "                with open(checkpoint_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    self.queue = data[\"queue\"]\n",
    "                    self.visited_listing = data[\"visited_listing\"]\n",
    "                    self.visited_detail = data[\"visited_detail\"]\n",
    "                    self.companies = data[\"companies\"]\n",
    "                    self.stats = data[\"stats\"]\n",
    "                    self.graph = data[\"graph\"]\n",
    "                print(f\"‚úì Resumed from checkpoint: {len(self.companies)} companies, {len(self.visited_listing)} pages\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Could not load checkpoint: {e}\")\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"Save progress to checkpoint\"\"\"\n",
    "        checkpoint_file = Config.CHECKPOINT_DIR / f\"{self.checkpoint_id}.pkl\"\n",
    "        try:\n",
    "            with open(checkpoint_file, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"queue\": self.queue,\n",
    "                    \"visited_listing\": self.visited_listing,\n",
    "                    \"visited_detail\": self.visited_detail,\n",
    "                    \"companies\": self.companies,\n",
    "                    \"stats\": self.stats,\n",
    "                    \"graph\": self.graph,\n",
    "                }, f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Checkpoint save failed: {e}\")\n",
    "\n",
    "    def crawl(self) -> Tuple[List[Company], CrawlStats, nx.DiGraph]:\n",
    "        \"\"\"Execute BFS crawl\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting BFS crawl: {self.seed_url}\")\n",
    "        print(f\"Domain: {self.domain}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        pages_crawled = 0\n",
    "        last_checkpoint = 0\n",
    "\n",
    "        while self.queue and pages_crawled < Config.MAX_PAGES_PER_SEED:\n",
    "            url, depth = self.queue.popleft()\n",
    "\n",
    "            # Skip if already visited or too deep\n",
    "            if url in self.visited_listing or depth > Config.MAX_DEPTH:\n",
    "                continue\n",
    "\n",
    "            self.visited_listing.add(url)\n",
    "\n",
    "            # Fetch page\n",
    "            soup = self.client.get_soup(url)\n",
    "            if not soup:\n",
    "                self.stats.errors += 1\n",
    "                continue\n",
    "\n",
    "            pages_crawled += 1\n",
    "            self.stats.pages_crawled += 1\n",
    "\n",
    "            # Extract companies from this page\n",
    "            jsonld_companies = CompanyExtractor.from_json_ld(soup, url)\n",
    "            block_companies, detail_links = CompanyExtractor.from_listing_blocks(soup, url)\n",
    "\n",
    "            self.companies.extend(jsonld_companies)\n",
    "            self.companies.extend(block_companies)\n",
    "            self.stats.companies_found = len(self.companies)\n",
    "\n",
    "            # Add detail links to graph and queue\n",
    "            for detail_url in detail_links[:Config.MAX_DETAIL_PAGES]:\n",
    "                if detail_url not in self.visited_detail:\n",
    "                    self.graph.add_edge(url, detail_url, type=\"detail\")\n",
    "                    self.visited_detail.add(detail_url)\n",
    "\n",
    "            # Discover new listing pages (same domain only)\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                if href.startswith((\"mailto:\", \"tel:\", \"javascript:\", \"#\")):\n",
    "                    continue\n",
    "\n",
    "                full_url = urljoin(url, href)\n",
    "\n",
    "                if (URLUtils.same_domain(full_url, self.seed_url) and\n",
    "                    full_url not in self.visited_listing and\n",
    "                    depth + 1 <= Config.MAX_DEPTH):\n",
    "\n",
    "                    self.queue.append((full_url, depth + 1))\n",
    "                    self.graph.add_edge(url, full_url, type=\"listing\")\n",
    "                    self.graph.add_node(full_url, type=\"listing\", depth=depth+1)\n",
    "\n",
    "            # Handle pagination\n",
    "            next_page = PaginationHandler.find_next_page(soup, url)\n",
    "            if next_page and next_page not in self.visited_listing:\n",
    "                self.queue.append((next_page, depth))  # Same depth for pagination\n",
    "                self.graph.add_edge(url, next_page, type=\"pagination\")\n",
    "\n",
    "            # Progress reporting\n",
    "            if pages_crawled % 10 == 0:\n",
    "                print(f\"  Progress: {pages_crawled} pages | {len(self.companies)} companies | \"\n",
    "                      f\"Queue: {len(self.queue)} | Depth: {depth}\")\n",
    "\n",
    "            # Checkpoint every N companies\n",
    "            if len(self.companies) - last_checkpoint >= Config.CHECKPOINT_INTERVAL:\n",
    "                self._save_checkpoint()\n",
    "                last_checkpoint = len(self.companies)\n",
    "\n",
    "            sleep_random()\n",
    "\n",
    "        # Fetch detail pages (parallel)\n",
    "        if self.visited_detail:\n",
    "            self._fetch_detail_pages()\n",
    "\n",
    "        # Finalize stats\n",
    "        self.stats.end_time = time.time()\n",
    "        self.stats.avg_response_time = self.client.avg_response_time()\n",
    "        self.stats.success_rate = self.client.success_rate()\n",
    "\n",
    "        # Final checkpoint\n",
    "        self._save_checkpoint()\n",
    "\n",
    "        print(f\"\\n‚úì Crawl complete!\")\n",
    "        print(f\"  Pages: {self.stats.pages_crawled}\")\n",
    "        print(f\"  Companies: {self.stats.companies_found}\")\n",
    "        print(f\"  Duration: {self.stats.duration()/60:.1f} min\")\n",
    "        print(f\"  Speed: {self.stats.pages_per_minute():.1f} pages/min\")\n",
    "\n",
    "        return self.companies, self.stats, self.graph\n",
    "\n",
    "    def _fetch_detail_pages(self):\n",
    "        \"\"\"Fetch detail pages in parallel\"\"\"\n",
    "        print(f\"\\nFetching {len(self.visited_detail)} detail pages...\")\n",
    "\n",
    "        detail_list = list(self.visited_detail)[:Config.MAX_DETAIL_PAGES]\n",
    "\n",
    "        def fetch_one(url):\n",
    "            soup = self.client.get_soup(url)\n",
    "            if soup:\n",
    "                return CompanyExtractor.from_detail_page(soup, url)\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=Config.ZYP_DETAIL_WORKERS) as executor:\n",
    "            futures = {executor.submit(fetch_one, url): url for url in detail_list}\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures), 1):\n",
    "                try:\n",
    "                    company = future.result()\n",
    "                    if company:\n",
    "                        self.companies.append(company)\n",
    "                        self.stats.companies_found += 1\n",
    "                    self.stats.detail_pages_crawled += 1\n",
    "                except Exception:\n",
    "                    self.stats.errors += 1\n",
    "\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"  Detail pages: {i}/{len(detail_list)} ({self.stats.companies_found} companies)\")\n",
    "\n",
    "# ============================================\n",
    "# ANALYTICS & VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "class Analytics:\n",
    "    \"\"\"Generate statistics and visualizations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_summary(all_stats: List[CrawlStats]) -> pd.DataFrame:\n",
    "        \"\"\"Create summary statistics DataFrame\"\"\"\n",
    "        summary_data = []\n",
    "\n",
    "        for stats in all_stats:\n",
    "            summary_data.append({\n",
    "                \"Seed URL\": stats.seed_url,\n",
    "                \"Duration (min)\": round(stats.duration() / 60, 2),\n",
    "                \"Pages Crawled\": stats.pages_crawled,\n",
    "                \"Detail Pages\": stats.detail_pages_crawled,\n",
    "                \"Companies Found\": stats.companies_found,\n",
    "                \"Pages/Min\": round(stats.pages_per_minute(), 2),\n",
    "                \"Avg Response (s)\": round(stats.avg_response_time, 3),\n",
    "                \"Success Rate %\": round(stats.success_rate, 1),\n",
    "                \"Errors\": stats.errors,\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_performance(all_stats: List[CrawlStats], output_file: Path):\n",
    "        \"\"\"Create performance visualization\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(\"Scraping Performance Analytics\", fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Extract data\n",
    "        seeds = [s.seed_url[:30] + \"...\" for s in all_stats]\n",
    "        pages = [s.pages_crawled for s in all_stats]\n",
    "        companies = [s.companies_found for s in all_stats]\n",
    "        speeds = [s.pages_per_minute() for s in all_stats]\n",
    "        success_rates = [s.success_rate for s in all_stats]\n",
    "\n",
    "        # Plot 1: Pages vs Companies\n",
    "        axes[0, 0].bar(range(len(seeds)), pages, alpha=0.7, label=\"Pages\", color='steelblue')\n",
    "        axes[0, 0].bar(range(len(seeds)), companies, alpha=0.7, label=\"Companies\", color='coral')\n",
    "        axes[0, 0].set_xlabel(\"Seed URL\")\n",
    "        axes[0, 0].set_ylabel(\"Count\")\n",
    "        axes[0, 0].set_title(\"Pages Crawled vs Companies Found\")\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_xticks(range(len(seeds)))\n",
    "        axes[0, 0].set_xticklabels(seeds, rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "        # Plot 2: Crawl Speed\n",
    "        axes[0, 1].plot(speeds, marker='o', linewidth=2, markersize=8, color='green')\n",
    "        axes[0, 1].set_xlabel(\"Seed Index\")\n",
    "        axes[0, 1].set_ylabel(\"Pages per Minute\")\n",
    "        axes[0, 1].set_title(\"Crawl Speed Performance\")\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axhline(sum(speeds)/len(speeds), color='red', linestyle='--',\n",
    "                           label=f'Avg: {sum(speeds)/len(speeds):.1f}')\n",
    "        axes[0, 1].legend()\n",
    "\n",
    "        # Plot 3: Success Rate\n",
    "        colors = ['green' if sr > 80 else 'orange' if sr > 60 else 'red' for sr in success_rates]\n",
    "        axes[1, 0].bar(range(len(seeds)), success_rates, color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_xlabel(\"Seed URL\")\n",
    "        axes[1, 0].set_ylabel(\"Success Rate %\")\n",
    "        axes[1, 0].set_title(\"Request Success Rate\")\n",
    "        axes[1, 0].set_xticks(range(len(seeds)))\n",
    "        axes[1, 0].set_xticklabels(seeds, rotation=45, ha='right', fontsize=8)\n",
    "        axes[1, 0].axhline(80, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Plot 4: Efficiency (Companies per Page)\n",
    "        efficiency = [c/p if p > 0 else 0 for c, p in zip(companies, pages)]\n",
    "        axes[1, 1].scatter(pages, companies, s=100, alpha=0.6, c=efficiency, cmap='viridis')\n",
    "        axes[1, 1].set_xlabel(\"Pages Crawled\")\n",
    "        axes[1, 1].set_ylabel(\"Companies Found\")\n",
    "        axes[1, 1].set_title(\"Extraction Efficiency\")\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Add trendline\n",
    "        if len(pages) > 1:\n",
    "            z = np.polyfit(pages, companies, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1, 1].plot(pages, p(pages), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Performance plot saved: {output_file}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_network(graph: nx.DiGraph, seed_url: str, output_file: Path):\n",
    "        \"\"\"Visualize BFS crawl network\"\"\"\n",
    "        if len(graph.nodes()) == 0:\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(16, 12))\n",
    "\n",
    "        # Layout\n",
    "        try:\n",
    "            pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n",
    "        except:\n",
    "            pos = nx.random_layout(graph)\n",
    "\n",
    "        # Node colors by type\n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        for node in graph.nodes():\n",
    "            node_type = graph.nodes[node].get('type', 'unknown')\n",
    "            if node_type == 'seed':\n",
    "                node_colors.append('red')\n",
    "                node_sizes.append(500)\n",
    "            elif node_type == 'listing':\n",
    "                node_colors.append('lightblue')\n",
    "                node_sizes.append(200)\n",
    "            elif node_type == 'detail':\n",
    "                node_colors.append('lightgreen')\n",
    "                node_sizes.append(150)\n",
    "            else:\n",
    "                node_colors.append('gray')\n",
    "                node_sizes.append(100)\n",
    "\n",
    "        # Draw edges by type\n",
    "        listing_edges = [(u, v) for u, v, d in graph.edges(data=True) if d.get('type') == 'listing']\n",
    "        detail_edges = [(u, v) for u, v, d in graph.edges(data=True) if d.get('type') == 'detail']\n",
    "        pagination_edges = [(u, v) for u, v, d in graph.edges(data=True) if d.get('type') == 'pagination']\n",
    "\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=listing_edges, edge_color='blue',\n",
    "                              alpha=0.3, arrows=True, arrowsize=10, width=1)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=detail_edges, edge_color='green',\n",
    "                              alpha=0.2, arrows=True, arrowsize=8, width=0.5)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=pagination_edges, edge_color='orange',\n",
    "                              alpha=0.4, arrows=True, arrowsize=10, width=1.5)\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color=node_colors, node_size=node_sizes, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"BFS Crawl Network Graph\\n{seed_url[:60]}...\", fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='red', label='Seed URL'),\n",
    "            Patch(facecolor='lightblue', label='Listing Pages'),\n",
    "            Patch(facecolor='lightgreen', label='Detail Pages'),\n",
    "            Patch(facecolor='blue', alpha=0.3, label='Listing Links'),\n",
    "            Patch(facecolor='green', alpha=0.3, label='Detail Links'),\n",
    "            Patch(facecolor='orange', alpha=0.3, label='Pagination'),\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Network graph saved: {output_file}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_data_quality(df: pd.DataFrame, output_file: Path):\n",
    "        \"\"\"Visualize data quality metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(\"Data Quality Analysis\", fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Completeness by field\n",
    "        completeness = {\n",
    "            'Name': (df['name'] != '').sum(),\n",
    "            'Address': (df['address'] != '').sum(),\n",
    "            'City': (df['city'] != '').sum(),\n",
    "            'Phone': (df['phone'] != '').sum(),\n",
    "            'Email': (df['email'] != '').sum(),\n",
    "            'Website': (df['website'] != '').sum(),\n",
    "            'Description': (df['description'] != '').sum(),\n",
    "        }\n",
    "\n",
    "        axes[0, 0].barh(list(completeness.keys()), list(completeness.values()), color='steelblue')\n",
    "        axes[0, 0].set_xlabel(\"Count\")\n",
    "        axes[0, 0].set_title(\"Field Completeness\")\n",
    "        axes[0, 0].axvline(len(df), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # City distribution (top 10)\n",
    "        city_counts = df['city'].value_counts().head(10)\n",
    "        axes[0, 1].bar(range(len(city_counts)), city_counts.values, color='coral')\n",
    "        axes[0, 1].set_xticks(range(len(city_counts)))\n",
    "        axes[0, 1].set_xticklabels(city_counts.index, rotation=45, ha='right')\n",
    "        axes[0, 1].set_ylabel(\"Count\")\n",
    "        axes[0, 1].set_title(\"Top 10 Cities\")\n",
    "\n",
    "        # Contact information availability\n",
    "        contact_data = {\n",
    "            'Phone Only': ((df['phone'] != '') & (df['email'] == '')).sum(),\n",
    "            'Email Only': ((df['email'] != '') & (df['phone'] == '')).sum(),\n",
    "            'Both': ((df['phone'] != '') & (df['email'] != '')).sum(),\n",
    "            'Neither': ((df['phone'] == '') & (df['email'] == '')).sum(),\n",
    "        }\n",
    "        axes[1, 0].pie(contact_data.values(), labels=contact_data.keys(), autopct='%1.1f%%',\n",
    "                       colors=['lightblue', 'lightgreen', 'gold', 'lightcoral'])\n",
    "        axes[1, 0].set_title(\"Contact Information Availability\")\n",
    "\n",
    "        # Records per source\n",
    "        source_counts = df['source_url'].value_counts().head(10)\n",
    "        axes[1, 1].barh(range(len(source_counts)), source_counts.values, color='green', alpha=0.6)\n",
    "        axes[1, 1].set_yticks(range(len(source_counts)))\n",
    "        axes[1, 1].set_yticklabels([url[:40] + \"...\" for url in source_counts.index], fontsize=8)\n",
    "        axes[1, 1].set_xlabel(\"Companies\")\n",
    "        axes[1, 1].set_title(\"Top 10 Sources\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Data quality plot saved: {output_file}\")\n",
    "\n",
    "# ============================================\n",
    "# MAIN ORCHESTRATOR\n",
    "# ============================================\n",
    "\n",
    "def extract_all_companies(directories_df: pd.DataFrame, max_workers: int = 2):\n",
    "    \"\"\"\n",
    "    Main function to extract companies from all directories\n",
    "\n",
    "    Args:\n",
    "        directories_df: DataFrame with 'listing_urls' column\n",
    "        max_workers: Number of parallel crawlers\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all extracted companies\n",
    "    \"\"\"\n",
    "    # Filter valid rows\n",
    "    df = directories_df.copy()\n",
    "    if \"has_listings\" in df.columns:\n",
    "        df = df[df[\"has_listings\"] == True]\n",
    "    df = df[df[\"listing_urls\"].notna() & (df[\"listing_urls\"].astype(str).str.strip() != \"\")]\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"‚ùå No valid listing URLs found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract seed URLs\n",
    "    all_seeds = []\n",
    "    for _, row in df.iterrows():\n",
    "        urls = [u.strip() for u in str(row[\"listing_urls\"]).split(\"|\") if u.strip()]\n",
    "        all_seeds.extend(urls)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ ZIMBABWE COMPANY EXTRACTOR\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total seed URLs: {len(all_seeds)}\")\n",
    "    print(f\"Parallel workers: {max_workers}\")\n",
    "    print(f\"Checkpoint directory: {Config.CHECKPOINT_DIR}\")\n",
    "    print(f\"Output directory: {Config.OUTPUT_DIR}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    all_companies = []\n",
    "    all_stats = []\n",
    "    all_graphs = []\n",
    "\n",
    "    # Process each seed URL\n",
    "    for i, seed_url in enumerate(all_seeds, 1):\n",
    "        print(f\"\\n[{i}/{len(all_seeds)}] Processing: {seed_url}\")\n",
    "\n",
    "        checkpoint_id = f\"seed_{i}_{URLUtils.get_domain(seed_url).replace('.', '_')}\"\n",
    "\n",
    "        try:\n",
    "            crawler = BFSCrawler(seed_url, checkpoint_id)\n",
    "            companies, stats, graph = crawler.crawl()\n",
    "\n",
    "            all_companies.extend(companies)\n",
    "            all_stats.append(stats)\n",
    "            all_graphs.append((seed_url, graph))\n",
    "\n",
    "            # Save incremental results\n",
    "            if companies:\n",
    "                temp_df = pd.DataFrame([asdict(c) for c in companies])\n",
    "                temp_file = Config.OUTPUT_DIR / f\"temp_{checkpoint_id}.csv\"\n",
    "                temp_df.to_csv(temp_file, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {seed_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Compile final results\n",
    "    if not all_companies:\n",
    "        print(\"\\n‚ùå No companies extracted\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä GENERATING FINAL RESULTS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    final_df = pd.DataFrame([asdict(c) for c in all_companies])\n",
    "\n",
    "    # Deduplicate\n",
    "    print(f\"Before deduplication: {len(final_df)} records\")\n",
    "    final_df = final_df.drop_duplicates(subset=['name', 'phone', 'website'], keep='first')\n",
    "    print(f\"After deduplication: {len(final_df)} records\")\n",
    "\n",
    "    # Save final CSV\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = Config.OUTPUT_DIR / f\"zimbabwe_companies_{timestamp}.csv\"\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n‚úì Final dataset saved: {output_file}\")\n",
    "\n",
    "    # Generate analytics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà GENERATING ANALYTICS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Summary statistics\n",
    "    summary_df = Analytics.generate_summary(all_stats)\n",
    "    summary_file = Config.STATS_DIR / f\"summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"‚úì Summary statistics: {summary_file}\")\n",
    "    print(f\"\\n{summary_df.to_string()}\\n\")\n",
    "\n",
    "    # Performance plots\n",
    "    perf_plot = Config.STATS_DIR / f\"performance_{timestamp}.png\"\n",
    "    Analytics.plot_performance(all_stats, perf_plot)\n",
    "\n",
    "    # Data quality plots\n",
    "    quality_plot = Config.STATS_DIR / f\"data_quality_{timestamp}.png\"\n",
    "    Analytics.plot_data_quality(final_df, quality_plot)\n",
    "\n",
    "    # Network graphs (first 5 seeds)\n",
    "    for i, (seed_url, graph) in enumerate(all_graphs[:5], 1):\n",
    "        network_file = Config.STATS_DIR / f\"network_seed{i}_{timestamp}.png\"\n",
    "        Analytics.plot_network(graph, seed_url, network_file)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ EXTRACTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total companies: {len(final_df)}\")\n",
    "    print(f\"Total pages crawled: {sum(s.pages_crawled for s in all_stats)}\")\n",
    "    print(f\"Total duration: {sum(s.duration() for s in all_stats)/60:.1f} minutes\")\n",
    "    print(f\"Average speed: {sum(s.pages_per_minute() for s in all_stats)/len(all_stats):.1f} pages/min\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# ============================================\n",
    "# EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load input data\n",
    "    try:\n",
    "        directories_with_listings\n",
    "    except NameError:\n",
    "        try:\n",
    "            directories_with_listings = pd.read_csv(\"directories_with_listings.csv\")\n",
    "            print(\"‚úì Loaded directories_with_listings.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Please load 'directories_with_listings' DataFrame\")\n",
    "            raise\n",
    "\n",
    "    # Run extraction\n",
    "    import numpy as np  # For trendline\n",
    "    companies_df = extract_all_companies(directories_with_listings, max_workers=2)\n",
    "\n",
    "    # Display sample\n",
    "    if not companies_df.empty:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SAMPLE RESULTS (First 10 records)\")\n",
    "        print(\"=\"*70)\n",
    "        print(companies_df.head(10).to_string())\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FIELD STATISTICS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Name:        {(companies_df['name'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Address:     {(companies_df['address'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"City:        {(companies_df['city'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Phone:       {(companies_df['phone'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Email:       {(companies_df['email'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Website:     {(companies_df['website'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Description: {(companies_df['description'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(\"=\"*70 + \"\\n\")"
   ],
   "id": "abf0d590caf57ea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ ZIMBABWE COMPANY EXTRACTOR\n",
      "======================================================================\n",
      "Total seed URLs: 7\n",
      "Parallel workers: 2\n",
      "Checkpoint directory: checkpoints\n",
      "Output directory: output\n",
      "======================================================================\n",
      "\n",
      "\n",
      "[1/7] Processing: https://afrikta.com/listing-locations/zimbabwe/\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://afrikta.com/listing-locations/zimbabwe/\n",
      "Domain: afrikta.com\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 3 companies | Queue: 120 | Depth: 1\n",
      "  Progress: 20 pages | 22 companies | Queue: 110 | Depth: 1\n",
      "  Progress: 30 pages | 39 companies | Queue: 95 | Depth: 1\n",
      "  Progress: 40 pages | 64 companies | Queue: 82 | Depth: 1\n",
      "  Progress: 50 pages | 71 companies | Queue: 79 | Depth: 1\n",
      "  Progress: 60 pages | 71 companies | Queue: 79 | Depth: 1\n",
      "  Progress: 70 pages | 71 companies | Queue: 79 | Depth: 1\n",
      "  Progress: 80 pages | 71 companies | Queue: 65 | Depth: 1\n",
      "  Progress: 90 pages | 150 companies | Queue: 51 | Depth: 1\n",
      "  Progress: 100 pages | 167 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 110 pages | 167 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 120 pages | 167 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 130 pages | 167 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 140 pages | 174 companies | Queue: 49 | Depth: 1\n",
      "  Progress: 150 pages | 188 companies | Queue: 49 | Depth: 1\n",
      "  Progress: 160 pages | 188 companies | Queue: 49 | Depth: 1\n",
      "  Progress: 170 pages | 188 companies | Queue: 49 | Depth: 1\n",
      "  Progress: 180 pages | 190 companies | Queue: 49 | Depth: 1\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6362bb90f96d106d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visuals",
   "id": "506cc48c04371193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53f9f2f29eb20b63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
