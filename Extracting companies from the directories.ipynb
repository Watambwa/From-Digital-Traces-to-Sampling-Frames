{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extracting companies from each directories",
   "id": "76fc00646abcf9b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Web Crawling on directories",
   "id": "aa1d42963910236a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T22:35:31.123854Z",
     "start_time": "2025-12-03T15:58:37.621589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "============================================\n",
    "Enhanced Zimbabwe Company Directory Scraper\n",
    "============================================\n",
    "Features:\n",
    "- Breadth-First Search (BFS) algorithm\n",
    "- Progressive checkpoint saving\n",
    "- Performance statistics & visualization\n",
    "- Network graph visualization\n",
    "- City and description extraction\n",
    "- Internet interruption resilient\n",
    "- Publication-ready analytics\n",
    "\n",
    "Author: Enhanced Version\n",
    "Date: 2025\n",
    "============================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Set, Dict, List, Optional, Tuple\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    # User agents for rotation\n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 Version/17.0 Safari/605.1.15\",\n",
    "    ]\n",
    "\n",
    "    # Request settings\n",
    "    REQUEST_TIMEOUT = 15\n",
    "    SLEEP_MIN, SLEEP_MAX = 0.3, 0.8  # Faster but respectful\n",
    "    RETRY_ATTEMPTS = 3\n",
    "\n",
    "    # Crawl limits (preventing infinite loops)\n",
    "    MAX_PAGES_PER_SEED = 3800          # Maximum pages per initial URL\n",
    "    MAX_DEPTH = 1                     # Maximum BFS depth\n",
    "    MAX_DETAIL_PAGES = 2000            # Maximum detail pages per seed\n",
    "    MAX_CATEGORIES = 800              # Maximum categories to explore\n",
    "\n",
    "    # ZimbabweYP specific settings\n",
    "    ZYP_MAX_PAGES_PER_CATEGORY = 100  # Reduced for speed\n",
    "    ZYP_DETAIL_WORKERS = 6            # Parallel detail fetching\n",
    "\n",
    "    # Checkpoint settings\n",
    "    CHECKPOINT_INTERVAL = 50          # Save every N companies\n",
    "    CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "    OUTPUT_DIR = Path(\"output\")\n",
    "    STATS_DIR = Path(\"statistics\")\n",
    "\n",
    "    # Country\n",
    "    COUNTRY = \"Zimbabwe\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [Config.CHECKPOINT_DIR, Config.OUTPUT_DIR, Config.STATS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# DATA STRUCTURES\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class Company:\n",
    "    \"\"\"Company data structure\"\"\"\n",
    "    name: str = \"\"\n",
    "    address: str = \"\"\n",
    "    city: str = \"\"\n",
    "    phone: str = \"\"\n",
    "    email: str = \"\"\n",
    "    website: str = \"\"\n",
    "    description: str = \"\"\n",
    "    source_url: str = \"\"\n",
    "    country: str = Config.COUNTRY\n",
    "    crawl_timestamp: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class CrawlStats:\n",
    "    \"\"\"Statistics for a crawl session\"\"\"\n",
    "    seed_url: str\n",
    "    start_time: float\n",
    "    end_time: float = 0\n",
    "    pages_crawled: int = 0\n",
    "    detail_pages_crawled: int = 0\n",
    "    companies_found: int = 0\n",
    "    errors: int = 0\n",
    "    avg_response_time: float = 0\n",
    "    success_rate: float = 0\n",
    "\n",
    "    def duration(self) -> float:\n",
    "        return self.end_time - self.start_time if self.end_time else time.time() - self.start_time\n",
    "\n",
    "    def pages_per_minute(self) -> float:\n",
    "        duration_min = self.duration() / 60\n",
    "        return self.pages_crawled / duration_min if duration_min > 0 else 0\n",
    "\n",
    "# ============================================\n",
    "# UTILITIES\n",
    "# ============================================\n",
    "\n",
    "class SessionManager:\n",
    "    \"\"\"Manages HTTP sessions with retry logic\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_session() -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=Config.RETRY_ATTEMPTS,\n",
    "            backoff_factor=0.3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=20)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    @staticmethod\n",
    "    def get_headers() -> Dict[str, str]:\n",
    "        return {\n",
    "            \"User-Agent\": random.choice(Config.USER_AGENTS),\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "        }\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Text cleaning and extraction utilities\"\"\"\n",
    "\n",
    "    PHONE_RE = re.compile(r\"(?:\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d)\")\n",
    "    EMAIL_RE = re.compile(r\"[A-Z0-9._%+\\-]+@[A-Z0-9.\\-]+\\.[A-Z]{2,}\", re.I)\n",
    "\n",
    "    # Zimbabwe city patterns\n",
    "    CITIES = [\n",
    "        \"Harare\", \"Bulawayo\", \"Chitungwiza\", \"Mutare\", \"Gweru\", \"Kwekwe\",\n",
    "        \"Kadoma\", \"Masvingo\", \"Chinhoyi\", \"Marondera\", \"Norton\", \"Chegutu\",\n",
    "        \"Bindura\", \"Beitbridge\", \"Redcliff\", \"Victoria Falls\", \"Hwange\",\n",
    "        \"Rusape\", \"Chiredzi\", \"Kariba\", \"Karoi\", \"Zvishavane\"\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def clean(text) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        return re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_phones(text: str) -> List[str]:\n",
    "        return list({TextProcessor.clean(m.group(0)) for m in TextProcessor.PHONE_RE.finditer(text or \"\")})\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emails(text: str) -> List[str]:\n",
    "        return list({TextProcessor.clean(m.group(0)) for m in TextProcessor.EMAIL_RE.finditer(text or \"\")})\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_city(text: str) -> str:\n",
    "        \"\"\"Extract city name from address/text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text_upper = text.upper()\n",
    "        for city in TextProcessor.CITIES:\n",
    "            if city.upper() in text_upper:\n",
    "                return city\n",
    "        return \"\"\n",
    "\n",
    "class URLUtils:\n",
    "    \"\"\"URL manipulation utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_domain(url: str) -> str:\n",
    "        try:\n",
    "            return urlparse(url).netloc.lower().replace(\"www.\", \"\")\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def same_domain(url1: str, url2: str) -> bool:\n",
    "        return URLUtils.get_domain(url1) == URLUtils.get_domain(url2)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(url: str) -> str:\n",
    "        try:\n",
    "            p = urlparse(url)\n",
    "            return f\"{p.scheme}://{p.netloc}{p.path or '/'}\"\n",
    "        except:\n",
    "            return url\n",
    "\n",
    "    @staticmethod\n",
    "    def is_zimbabwe_yp(url: str) -> bool:\n",
    "        return \"zimbabweyp.com\" in URLUtils.get_domain(url)\n",
    "\n",
    "def sleep_random():\n",
    "    \"\"\"Random sleep to avoid rate limiting\"\"\"\n",
    "    time.sleep(random.uniform(Config.SLEEP_MIN, Config.SLEEP_MAX))\n",
    "\n",
    "# ============================================\n",
    "# HTTP & PARSING\n",
    "# ============================================\n",
    "\n",
    "class HTTPClient:\n",
    "    \"\"\"HTTP client with timing and error tracking\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = SessionManager.create_session()\n",
    "        self.response_times = []\n",
    "        self.errors = 0\n",
    "        self.success = 0\n",
    "\n",
    "    def get_soup(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Fetch URL and return BeautifulSoup object\"\"\"\n",
    "        start = time.time()\n",
    "        try:\n",
    "            resp = self.session.get(\n",
    "                url,\n",
    "                headers=SessionManager.get_headers(),\n",
    "                timeout=Config.REQUEST_TIMEOUT,\n",
    "                allow_redirects=True\n",
    "            )\n",
    "\n",
    "            if resp.status_code >= 400:\n",
    "                self.errors += 1\n",
    "                return None\n",
    "\n",
    "            if not resp.encoding:\n",
    "                resp.encoding = \"utf-8\"\n",
    "\n",
    "            self.response_times.append(time.time() - start)\n",
    "            self.success += 1\n",
    "            return BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.errors += 1\n",
    "            return None\n",
    "\n",
    "    def avg_response_time(self) -> float:\n",
    "        return sum(self.response_times) / len(self.response_times) if self.response_times else 0\n",
    "\n",
    "    def success_rate(self) -> float:\n",
    "        total = self.success + self.errors\n",
    "        return (self.success / total * 100) if total > 0 else 0\n",
    "\n",
    "# ============================================\n",
    "# EXTRACTION LOGIC\n",
    "# ============================================\n",
    "\n",
    "class CompanyExtractor:\n",
    "    \"\"\"Extract company information from pages\"\"\"\n",
    "\n",
    "    ORG_TYPES = {\"Organization\", \"LocalBusiness\", \"Corporation\", \"Company\", \"NGO\", \"Store\"}\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json_ld(soup: BeautifulSoup, source_url: str) -> List[Company]:\n",
    "        \"\"\"Extract companies from JSON-LD structured data\"\"\"\n",
    "        companies = []\n",
    "\n",
    "        for tag in soup.find_all(\"script\", type=lambda t: t and \"ld+json\" in t):\n",
    "            try:\n",
    "                data = json.loads(tag.string or \"\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            items = []\n",
    "            if isinstance(data, list):\n",
    "                items = data\n",
    "            elif isinstance(data, dict):\n",
    "                if \"@graph\" in data:\n",
    "                    items = data[\"@graph\"]\n",
    "                else:\n",
    "                    items = [data]\n",
    "\n",
    "            for item in items:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "\n",
    "                item_type = item.get(\"@type\", \"\")\n",
    "                types = {item_type} if isinstance(item_type, str) else set(item_type or [])\n",
    "\n",
    "                if not types.intersection(CompanyExtractor.ORG_TYPES):\n",
    "                    continue\n",
    "\n",
    "                name = TextProcessor.clean(item.get(\"name\", \"\"))\n",
    "                if not name:\n",
    "                    continue\n",
    "\n",
    "                # Parse address\n",
    "                addr = item.get(\"address\", \"\")\n",
    "                if isinstance(addr, dict):\n",
    "                    addr_parts = [\n",
    "                        addr.get(\"streetAddress\", \"\"),\n",
    "                        addr.get(\"addressLocality\", \"\"),\n",
    "                        addr.get(\"addressRegion\", \"\"),\n",
    "                        addr.get(\"postalCode\", \"\"),\n",
    "                    ]\n",
    "                    address = TextProcessor.clean(\" \".join(p for p in addr_parts if p))\n",
    "                else:\n",
    "                    address = TextProcessor.clean(addr)\n",
    "\n",
    "                company = Company(\n",
    "                    name=name,\n",
    "                    address=address,\n",
    "                    city=TextProcessor.extract_city(address),\n",
    "                    phone=TextProcessor.clean(item.get(\"telephone\", \"\")),\n",
    "                    email=TextProcessor.clean(item.get(\"email\", \"\")),\n",
    "                    website=TextProcessor.clean(item.get(\"url\", \"\")),\n",
    "                    description=TextProcessor.clean(item.get(\"description\", \"\")),\n",
    "                    source_url=source_url,\n",
    "                    crawl_timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "                companies.append(company)\n",
    "\n",
    "        return companies\n",
    "\n",
    "    @staticmethod\n",
    "    def from_listing_blocks(soup: BeautifulSoup, base_url: str) -> Tuple[List[Company], List[str]]:\n",
    "        \"\"\"Extract companies and detail links from listing blocks\"\"\"\n",
    "        companies = []\n",
    "        detail_links = set()\n",
    "\n",
    "        # Common listing selectors\n",
    "        selectors = [\n",
    "            'div[class*=\"listing\"]', 'div[class*=\"result\"]', 'div[class*=\"company\"]',\n",
    "            'div[class*=\"business\"]', 'li[class*=\"listing\"]', 'article', '.card'\n",
    "        ]\n",
    "\n",
    "        blocks = []\n",
    "        for sel in selectors:\n",
    "            blocks.extend(soup.select(sel))\n",
    "\n",
    "        for block in blocks[:100]:  # Limit to avoid over-processing\n",
    "            try:\n",
    "                # Extract name\n",
    "                name_elem = (\n",
    "                    block.select_one('h2 a, h3 a, h4 a, .name a, .title a') or\n",
    "                    block.select_one('h2, h3, h4, .name, .title, strong')\n",
    "                )\n",
    "                name = TextProcessor.clean(name_elem.get_text()) if name_elem else \"\"\n",
    "\n",
    "                if not name:\n",
    "                    continue\n",
    "\n",
    "                # Extract address\n",
    "                addr_elem = block.select_one('address, .address, .location, [itemprop=\"address\"]')\n",
    "                address = TextProcessor.clean(addr_elem.get_text()) if addr_elem else \"\"\n",
    "\n",
    "                # Extract contact info\n",
    "                phone_elem = block.select_one('a[href^=\"tel:\"], .phone, [itemprop=\"telephone\"]')\n",
    "                phone = TextProcessor.clean(phone_elem.get_text()) if phone_elem else \"\"\n",
    "\n",
    "                email_elem = block.select_one('a[href^=\"mailto:\"], .email')\n",
    "                email = TextProcessor.clean(email_elem.get_text()) if email_elem else \"\"\n",
    "\n",
    "                # Extract description\n",
    "                desc_elem = block.select_one('.description, .about, .summary, p')\n",
    "                description = TextProcessor.clean(desc_elem.get_text()[:500]) if desc_elem else \"\"\n",
    "\n",
    "                # Find links\n",
    "                website = \"\"\n",
    "                detail_url = None\n",
    "\n",
    "                for a in block.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if href.startswith((\"mailto:\", \"tel:\", \"javascript:\", \"#\")):\n",
    "                        continue\n",
    "\n",
    "                    full_url = urljoin(base_url, href)\n",
    "\n",
    "                    if not URLUtils.same_domain(full_url, base_url):\n",
    "                        website = full_url\n",
    "                    elif not detail_url:\n",
    "                        detail_url = full_url\n",
    "\n",
    "                # Fallback: extract from block text\n",
    "                block_text = block.get_text()\n",
    "                if not phone:\n",
    "                    phones = TextProcessor.extract_phones(block_text)\n",
    "                    phone = phones[0] if phones else \"\"\n",
    "                if not email:\n",
    "                    emails = TextProcessor.extract_emails(block_text)\n",
    "                    email = emails[0] if emails else \"\"\n",
    "\n",
    "                company = Company(\n",
    "                    name=name,\n",
    "                    address=address,\n",
    "                    city=TextProcessor.extract_city(address),\n",
    "                    phone=phone,\n",
    "                    email=email,\n",
    "                    website=website,\n",
    "                    description=description,\n",
    "                    source_url=detail_url or base_url,\n",
    "                    crawl_timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "                companies.append(company)\n",
    "\n",
    "                if detail_url:\n",
    "                    detail_links.add(detail_url)\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        return companies, list(detail_links)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_detail_page(soup: BeautifulSoup, url: str) -> Optional[Company]:\n",
    "        \"\"\"Extract company from detail page\"\"\"\n",
    "        # Try JSON-LD first\n",
    "        jsonld_companies = CompanyExtractor.from_json_ld(soup, url)\n",
    "        if jsonld_companies:\n",
    "            return jsonld_companies[0]\n",
    "\n",
    "        # Heuristic extraction\n",
    "        name_elem = soup.select_one('h1, .company-name, .page-title, [itemprop=\"name\"]')\n",
    "        name = TextProcessor.clean(name_elem.get_text()) if name_elem else \"\"\n",
    "\n",
    "        if not name:\n",
    "            title = soup.find(\"title\")\n",
    "            name = TextProcessor.clean(title.get_text()) if title else \"\"\n",
    "\n",
    "        addr_elem = soup.select_one('address, .address, .location, [itemprop=\"address\"]')\n",
    "        address = TextProcessor.clean(addr_elem.get_text()) if addr_elem else \"\"\n",
    "\n",
    "        # Extract description/about\n",
    "        desc_elem = soup.select_one('.about, .description, .company-info, [itemprop=\"description\"]')\n",
    "        description = \"\"\n",
    "        if desc_elem:\n",
    "            description = TextProcessor.clean(desc_elem.get_text()[:1000])\n",
    "\n",
    "        # Extract from full text\n",
    "        page_text = soup.get_text()\n",
    "        phones = TextProcessor.extract_phones(page_text)\n",
    "        emails = TextProcessor.extract_emails(page_text)\n",
    "\n",
    "        # Find website\n",
    "        website = \"\"\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            text = a.get_text().lower()\n",
    "            if \"website\" in text or \"visit\" in text:\n",
    "                website = urljoin(url, a[\"href\"])\n",
    "                break\n",
    "\n",
    "        return Company(\n",
    "            name=name,\n",
    "            address=address,\n",
    "            city=TextProcessor.extract_city(address),\n",
    "            phone=phones[0] if phones else \"\",\n",
    "            email=emails[0] if emails else \"\",\n",
    "            website=website,\n",
    "            description=description,\n",
    "            source_url=url,\n",
    "            crawl_timestamp=datetime.now().isoformat()\n",
    "        ) if name else None\n",
    "\n",
    "# ============================================\n",
    "# PAGINATION\n",
    "# ============================================\n",
    "\n",
    "class PaginationHandler:\n",
    "    \"\"\"Handle pagination across different sites\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def find_next_page(soup: BeautifulSoup, current_url: str) -> Optional[str]:\n",
    "        \"\"\"Find the next page URL\"\"\"\n",
    "        # Check rel=\"next\"\n",
    "        for a in soup.find_all(\"a\", href=True, rel=True):\n",
    "            rels = a.get(\"rel\")\n",
    "            if isinstance(rels, str):\n",
    "                rels = [rels]\n",
    "            if any(\"next\" in r.lower() for r in (rels or [])):\n",
    "                return urljoin(current_url, a[\"href\"])\n",
    "\n",
    "        # Check text/aria labels\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            text = a.get_text().lower()\n",
    "            classes = \" \".join(a.get(\"class\", [])).lower()\n",
    "            aria = (a.get(\"aria-label\") or \"\").lower()\n",
    "\n",
    "            if any(term in text for term in [\"next\", \"¬ª\", \">\"]) or \"next\" in classes or \"next\" in aria:\n",
    "                return urljoin(current_url, a[\"href\"])\n",
    "\n",
    "        # Check URL patterns\n",
    "        # Pattern: ?page=N\n",
    "        match = re.search(r\"([?&])(page|p)=(\\d+)\", current_url, re.I)\n",
    "        if match:\n",
    "            next_num = int(match.group(3)) + 1\n",
    "            return re.sub(r\"([?&])(page|p)=\\d+\", rf\"\\1\\2={next_num}\", current_url)\n",
    "\n",
    "        # Pattern: /page/N/\n",
    "        match = re.search(r\"/page/(\\d+)/?$\", current_url, re.I)\n",
    "        if match:\n",
    "            next_num = int(match.group(1)) + 1\n",
    "            return re.sub(r\"/page/\\d+/?$\", f\"/page/{next_num}/\", current_url)\n",
    "\n",
    "        return None\n",
    "\n",
    "# ============================================\n",
    "# BFS CRAWLER\n",
    "# ============================================\n",
    "\n",
    "class BFSCrawler:\n",
    "    \"\"\"Breadth-First Search web crawler with checkpointing\"\"\"\n",
    "\n",
    "    def __init__(self, seed_url: str, checkpoint_id: str):\n",
    "        self.seed_url = seed_url\n",
    "        self.checkpoint_id = checkpoint_id\n",
    "        self.domain = URLUtils.get_domain(seed_url)\n",
    "\n",
    "        # BFS data structures\n",
    "        self.queue = deque([(seed_url, 0)])  # (url, depth)\n",
    "        self.visited_listing = set()\n",
    "        self.visited_detail = set()\n",
    "\n",
    "        # Results\n",
    "        self.companies: List[Company] = []\n",
    "        self.stats = CrawlStats(seed_url=seed_url, start_time=time.time())\n",
    "\n",
    "        # Network graph for visualization\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.graph.add_node(seed_url, type=\"seed\", depth=0)\n",
    "\n",
    "        # HTTP client\n",
    "        self.client = HTTPClient()\n",
    "\n",
    "        # Load checkpoint if exists\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        \"\"\"Load progress from checkpoint\"\"\"\n",
    "        checkpoint_file = Config.CHECKPOINT_DIR / f\"{self.checkpoint_id}.pkl\"\n",
    "        if checkpoint_file.exists():\n",
    "            try:\n",
    "                with open(checkpoint_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    self.queue = data[\"queue\"]\n",
    "                    self.visited_listing = data[\"visited_listing\"]\n",
    "                    self.visited_detail = data[\"visited_detail\"]\n",
    "                    self.companies = data[\"companies\"]\n",
    "                    self.stats = data[\"stats\"]\n",
    "                    self.graph = data[\"graph\"]\n",
    "                print(f\"‚úì Resumed from checkpoint: {len(self.companies)} companies, {len(self.visited_listing)} pages\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Could not load checkpoint: {e}\")\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"Save progress to checkpoint\"\"\"\n",
    "        checkpoint_file = Config.CHECKPOINT_DIR / f\"{self.checkpoint_id}.pkl\"\n",
    "        try:\n",
    "            with open(checkpoint_file, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"queue\": self.queue,\n",
    "                    \"visited_listing\": self.visited_listing,\n",
    "                    \"visited_detail\": self.visited_detail,\n",
    "                    \"companies\": self.companies,\n",
    "                    \"stats\": self.stats,\n",
    "                    \"graph\": self.graph,\n",
    "                }, f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Checkpoint save failed: {e}\")\n",
    "\n",
    "    def crawl(self) -> Tuple[List[Company], CrawlStats, nx.DiGraph]:\n",
    "        \"\"\"Execute BFS crawl\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting BFS crawl: {self.seed_url}\")\n",
    "        print(f\"Domain: {self.domain}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        pages_crawled = 0\n",
    "        last_checkpoint = 0\n",
    "\n",
    "        while self.queue and pages_crawled < Config.MAX_PAGES_PER_SEED:\n",
    "            url, depth = self.queue.popleft()\n",
    "\n",
    "            # Skip if already visited or too deep\n",
    "            if url in self.visited_listing or depth > Config.MAX_DEPTH:\n",
    "                continue\n",
    "\n",
    "            self.visited_listing.add(url)\n",
    "\n",
    "            # Fetch page\n",
    "            soup = self.client.get_soup(url)\n",
    "            if not soup:\n",
    "                self.stats.errors += 1\n",
    "                continue\n",
    "\n",
    "            pages_crawled += 1\n",
    "            self.stats.pages_crawled += 1\n",
    "\n",
    "            # Extract companies from this page\n",
    "            jsonld_companies = CompanyExtractor.from_json_ld(soup, url)\n",
    "            block_companies, detail_links = CompanyExtractor.from_listing_blocks(soup, url)\n",
    "\n",
    "            self.companies.extend(jsonld_companies)\n",
    "            self.companies.extend(block_companies)\n",
    "            self.stats.companies_found = len(self.companies)\n",
    "\n",
    "            # Add detail links to graph and queue\n",
    "            for detail_url in detail_links[:Config.MAX_DETAIL_PAGES]:\n",
    "                if detail_url not in self.visited_detail:\n",
    "                    self.graph.add_edge(url, detail_url, type=\"detail\")\n",
    "                    self.visited_detail.add(detail_url)\n",
    "\n",
    "            # Discover new listing pages (same domain only)\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                if href.startswith((\"mailto:\", \"tel:\", \"javascript:\", \"#\")):\n",
    "                    continue\n",
    "\n",
    "                full_url = urljoin(url, href)\n",
    "\n",
    "                if (URLUtils.same_domain(full_url, self.seed_url) and\n",
    "                    full_url not in self.visited_listing and\n",
    "                    depth + 1 <= Config.MAX_DEPTH):\n",
    "\n",
    "                    self.queue.append((full_url, depth + 1))\n",
    "                    self.graph.add_edge(url, full_url, type=\"listing\")\n",
    "                    self.graph.add_node(full_url, type=\"listing\", depth=depth+1)\n",
    "\n",
    "            # Handle pagination\n",
    "            next_page = PaginationHandler.find_next_page(soup, url)\n",
    "            if next_page and next_page not in self.visited_listing:\n",
    "                self.queue.append((next_page, depth))  # Same depth for pagination\n",
    "                self.graph.add_edge(url, next_page, type=\"pagination\")\n",
    "\n",
    "            # Progress reporting\n",
    "            if pages_crawled % 10 == 0:\n",
    "                print(f\"  Progress: {pages_crawled} pages | {len(self.companies)} companies | \"\n",
    "                      f\"Queue: {len(self.queue)} | Depth: {depth}\")\n",
    "\n",
    "            # Checkpoint every N companies\n",
    "            if len(self.companies) - last_checkpoint >= Config.CHECKPOINT_INTERVAL:\n",
    "                self._save_checkpoint()\n",
    "                last_checkpoint = len(self.companies)\n",
    "\n",
    "            sleep_random()\n",
    "\n",
    "        # Fetch detail pages (parallel)\n",
    "        if self.visited_detail:\n",
    "            self._fetch_detail_pages()\n",
    "\n",
    "        # Finalize stats\n",
    "        self.stats.end_time = time.time()\n",
    "        self.stats.avg_response_time = self.client.avg_response_time()\n",
    "        self.stats.success_rate = self.client.success_rate()\n",
    "\n",
    "        # Final checkpoint\n",
    "        self._save_checkpoint()\n",
    "\n",
    "        print(f\"\\n‚úì Crawl complete!\")\n",
    "        print(f\"  Pages: {self.stats.pages_crawled}\")\n",
    "        print(f\"  Companies: {self.stats.companies_found}\")\n",
    "        print(f\"  Duration: {self.stats.duration()/60:.1f} min\")\n",
    "        print(f\"  Speed: {self.stats.pages_per_minute():.1f} pages/min\")\n",
    "\n",
    "        return self.companies, self.stats, self.graph\n",
    "\n",
    "    def _fetch_detail_pages(self):\n",
    "        \"\"\"Fetch detail pages in parallel\"\"\"\n",
    "        print(f\"\\nFetching {len(self.visited_detail)} detail pages...\")\n",
    "\n",
    "        detail_list = list(self.visited_detail)[:Config.MAX_DETAIL_PAGES]\n",
    "\n",
    "        def fetch_one(url):\n",
    "            soup = self.client.get_soup(url)\n",
    "            if soup:\n",
    "                return CompanyExtractor.from_detail_page(soup, url)\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=Config.ZYP_DETAIL_WORKERS) as executor:\n",
    "            futures = {executor.submit(fetch_one, url): url for url in detail_list}\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures), 1):\n",
    "                try:\n",
    "                    company = future.result()\n",
    "                    if company:\n",
    "                        self.companies.append(company)\n",
    "                        self.stats.companies_found += 1\n",
    "                    self.stats.detail_pages_crawled += 1\n",
    "                except Exception:\n",
    "                    self.stats.errors += 1\n",
    "\n",
    "                if i % 50 == 0:\n",
    "                    print(f\"  Detail pages: {i}/{len(detail_list)} ({self.stats.companies_found} companies)\")\n",
    "\n",
    "# ============================================\n",
    "# ANALYTICS & VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "class Analytics:\n",
    "    \"\"\"Generate statistics and visualizations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_summary(all_stats: List[CrawlStats]) -> pd.DataFrame:\n",
    "        \"\"\"Create summary statistics DataFrame\"\"\"\n",
    "        summary_data = []\n",
    "\n",
    "        for stats in all_stats:\n",
    "            summary_data.append({\n",
    "                \"Seed URL\": stats.seed_url,\n",
    "                \"Duration (min)\": round(stats.duration() / 60, 2),\n",
    "                \"Pages Crawled\": stats.pages_crawled,\n",
    "                \"Detail Pages\": stats.detail_pages_crawled,\n",
    "                \"Companies Found\": stats.companies_found,\n",
    "                \"Pages/Min\": round(stats.pages_per_minute(), 2),\n",
    "                \"Avg Response (s)\": round(stats.avg_response_time, 3),\n",
    "                \"Success Rate %\": round(stats.success_rate, 1),\n",
    "                \"Errors\": stats.errors,\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_performance(all_stats: List[CrawlStats], output_file: Path):\n",
    "        \"\"\"Create performance visualization\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(\"Scraping Performance Analytics\", fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Extract data\n",
    "        seeds = [s.seed_url[:30] + \"...\" for s in all_stats]\n",
    "        pages = [s.pages_crawled for s in all_stats]\n",
    "        companies = [s.companies_found for s in all_stats]\n",
    "        speeds = [s.pages_per_minute() for s in all_stats]\n",
    "        success_rates = [s.success_rate for s in all_stats]\n",
    "\n",
    "        # Plot 1: Pages vs Companies\n",
    "        axes[0, 0].bar(range(len(seeds)), pages, alpha=0.7, label=\"Pages\", color='steelblue')\n",
    "        axes[0, 0].bar(range(len(seeds)), companies, alpha=0.7, label=\"Companies\", color='coral')\n",
    "        axes[0, 0].set_xlabel(\"Seed URL\")\n",
    "        axes[0, 0].set_ylabel(\"Count\")\n",
    "        axes[0, 0].set_title(\"Pages Crawled vs Companies Found\")\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_xticks(range(len(seeds)))\n",
    "        axes[0, 0].set_xticklabels(seeds, rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "        # Plot 2: Crawl Speed\n",
    "        axes[0, 1].plot(speeds, marker='o', linewidth=2, markersize=8, color='green')\n",
    "        axes[0, 1].set_xlabel(\"Seed Index\")\n",
    "        axes[0, 1].set_ylabel(\"Pages per Minute\")\n",
    "        axes[0, 1].set_title(\"Crawl Speed Performance\")\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axhline(sum(speeds)/len(speeds), color='red', linestyle='--',\n",
    "                           label=f'Avg: {sum(speeds)/len(speeds):.1f}')\n",
    "        axes[0, 1].legend()\n",
    "\n",
    "        # Plot 3: Success Rate\n",
    "        colors = ['green' if sr > 80 else 'orange' if sr > 60 else 'red' for sr in success_rates]\n",
    "        axes[1, 0].bar(range(len(seeds)), success_rates, color=colors, alpha=0.7)\n",
    "        axes[1, 0].set_xlabel(\"Seed URL\")\n",
    "        axes[1, 0].set_ylabel(\"Success Rate %\")\n",
    "        axes[1, 0].set_title(\"Request Success Rate\")\n",
    "        axes[1, 0].set_xticks(range(len(seeds)))\n",
    "        axes[1, 0].set_xticklabels(seeds, rotation=45, ha='right', fontsize=8)\n",
    "        axes[1, 0].axhline(80, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Plot 4: Efficiency (Companies per Page)\n",
    "        efficiency = [c/p if p > 0 else 0 for c, p in zip(companies, pages)]\n",
    "        axes[1, 1].scatter(pages, companies, s=100, alpha=0.6, c=efficiency, cmap='viridis')\n",
    "        axes[1, 1].set_xlabel(\"Pages Crawled\")\n",
    "        axes[1, 1].set_ylabel(\"Companies Found\")\n",
    "        axes[1, 1].set_title(\"Extraction Efficiency\")\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Add trendline\n",
    "        if len(pages) > 1:\n",
    "            z = np.polyfit(pages, companies, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1, 1].plot(pages, p(pages), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Performance plot saved: {output_file}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_network(graph: nx.DiGraph, seed_url: str, output_file: Path):\n",
    "        \"\"\"Visualize BFS crawl network\"\"\"\n",
    "        if len(graph.nodes()) == 0:\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(16, 12))\n",
    "\n",
    "        # Layout\n",
    "        try:\n",
    "            pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n",
    "        except:\n",
    "            pos = nx.random_layout(graph)\n",
    "\n",
    "        # Node colors by type\n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        for node in graph.nodes():\n",
    "            node_type = graph.nodes[node].get('type', 'unknown')\n",
    "            if node_type == 'seed':\n",
    "                node_colors.append('red')\n",
    "                node_sizes.append(500)\n",
    "            elif node_type == 'listing':\n",
    "                node_colors.append('lightblue')\n",
    "                node_sizes.append(200)\n",
    "            elif node_type == 'detail':\n",
    "                node_colors.append('lightgreen')\n",
    "                node_sizes.append(150)\n",
    "            else:\n",
    "                node_colors.append('gray')\n",
    "                node_sizes.append(100)\n",
    "\n",
    "        # Draw edges by type\n",
    "        listing_edges = [(u, v) for u, v, d in graph.edges(data=True) if d.get('type') == 'listing']\n",
    "        detail_edges = [(u, v) for u, v, d in graph.edges(data=True) if d.get('type') == 'detail']\n",
    "        pagination_edges = [(u, v) for u, v, d in graph.edges(data=True) if d.get('type') == 'pagination']\n",
    "\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=listing_edges, edge_color='blue',\n",
    "                              alpha=0.3, arrows=True, arrowsize=10, width=1)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=detail_edges, edge_color='green',\n",
    "                              alpha=0.2, arrows=True, arrowsize=8, width=0.5)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=pagination_edges, edge_color='orange',\n",
    "                              alpha=0.4, arrows=True, arrowsize=10, width=1.5)\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color=node_colors, node_size=node_sizes, alpha=0.7)\n",
    "\n",
    "        plt.title(f\"BFS Crawl Network Graph\\n{seed_url[:60]}...\", fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='red', label='Seed URL'),\n",
    "            Patch(facecolor='lightblue', label='Listing Pages'),\n",
    "            Patch(facecolor='lightgreen', label='Detail Pages'),\n",
    "            Patch(facecolor='blue', alpha=0.3, label='Listing Links'),\n",
    "            Patch(facecolor='green', alpha=0.3, label='Detail Links'),\n",
    "            Patch(facecolor='orange', alpha=0.3, label='Pagination'),\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Network graph saved: {output_file}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_data_quality(df: pd.DataFrame, output_file: Path):\n",
    "        \"\"\"Visualize data quality metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle(\"Data Quality Analysis\", fontsize=16, fontweight='bold')\n",
    "\n",
    "        # Completeness by field\n",
    "        completeness = {\n",
    "            'Name': (df['name'] != '').sum(),\n",
    "            'Address': (df['address'] != '').sum(),\n",
    "            'City': (df['city'] != '').sum(),\n",
    "            'Phone': (df['phone'] != '').sum(),\n",
    "            'Email': (df['email'] != '').sum(),\n",
    "            'Website': (df['website'] != '').sum(),\n",
    "            'Description': (df['description'] != '').sum(),\n",
    "        }\n",
    "\n",
    "        axes[0, 0].barh(list(completeness.keys()), list(completeness.values()), color='steelblue')\n",
    "        axes[0, 0].set_xlabel(\"Count\")\n",
    "        axes[0, 0].set_title(\"Field Completeness\")\n",
    "        axes[0, 0].axvline(len(df), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # City distribution (top 10)\n",
    "        city_counts = df['city'].value_counts().head(10)\n",
    "        axes[0, 1].bar(range(len(city_counts)), city_counts.values, color='coral')\n",
    "        axes[0, 1].set_xticks(range(len(city_counts)))\n",
    "        axes[0, 1].set_xticklabels(city_counts.index, rotation=45, ha='right')\n",
    "        axes[0, 1].set_ylabel(\"Count\")\n",
    "        axes[0, 1].set_title(\"Top 10 Cities\")\n",
    "\n",
    "        # Contact information availability\n",
    "        contact_data = {\n",
    "            'Phone Only': ((df['phone'] != '') & (df['email'] == '')).sum(),\n",
    "            'Email Only': ((df['email'] != '') & (df['phone'] == '')).sum(),\n",
    "            'Both': ((df['phone'] != '') & (df['email'] != '')).sum(),\n",
    "            'Neither': ((df['phone'] == '') & (df['email'] == '')).sum(),\n",
    "        }\n",
    "        axes[1, 0].pie(contact_data.values(), labels=contact_data.keys(), autopct='%1.1f%%',\n",
    "                       colors=['lightblue', 'lightgreen', 'gold', 'lightcoral'])\n",
    "        axes[1, 0].set_title(\"Contact Information Availability\")\n",
    "\n",
    "        # Records per source\n",
    "        source_counts = df['source_url'].value_counts().head(10)\n",
    "        axes[1, 1].barh(range(len(source_counts)), source_counts.values, color='green', alpha=0.6)\n",
    "        axes[1, 1].set_yticks(range(len(source_counts)))\n",
    "        axes[1, 1].set_yticklabels([url[:40] + \"...\" for url in source_counts.index], fontsize=8)\n",
    "        axes[1, 1].set_xlabel(\"Companies\")\n",
    "        axes[1, 1].set_title(\"Top 10 Sources\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Data quality plot saved: {output_file}\")\n",
    "\n",
    "# ============================================\n",
    "# MAIN ORCHESTRATOR\n",
    "# ============================================\n",
    "\n",
    "def extract_all_companies(directories_df: pd.DataFrame, max_workers: int = 2):\n",
    "    \"\"\"\n",
    "    Main function to extract companies from all directories\n",
    "\n",
    "    Args:\n",
    "        directories_df: DataFrame with 'listing_urls' column\n",
    "        max_workers: Number of parallel crawlers\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all extracted companies\n",
    "    \"\"\"\n",
    "    # Filter valid rows\n",
    "    df = directories_df.copy()\n",
    "    if \"has_listings\" in df.columns:\n",
    "        df = df[df[\"has_listings\"] == True]\n",
    "    df = df[df[\"listing_urls\"].notna() & (df[\"listing_urls\"].astype(str).str.strip() != \"\")]\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"‚ùå No valid listing URLs found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract seed URLs\n",
    "    all_seeds = []\n",
    "    for _, row in df.iterrows():\n",
    "        urls = [u.strip() for u in str(row[\"listing_urls\"]).split(\"|\") if u.strip()]\n",
    "        all_seeds.extend(urls)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ ZIMBABWE COMPANY EXTRACTOR\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total seed URLs: {len(all_seeds)}\")\n",
    "    print(f\"Parallel workers: {max_workers}\")\n",
    "    print(f\"Checkpoint directory: {Config.CHECKPOINT_DIR}\")\n",
    "    print(f\"Output directory: {Config.OUTPUT_DIR}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    all_companies = []\n",
    "    all_stats = []\n",
    "    all_graphs = []\n",
    "\n",
    "    # Process each seed URL\n",
    "    for i, seed_url in enumerate(all_seeds, 1):\n",
    "        print(f\"\\n[{i}/{len(all_seeds)}] Processing: {seed_url}\")\n",
    "\n",
    "        checkpoint_id = f\"seed_{i}_{URLUtils.get_domain(seed_url).replace('.', '_')}\"\n",
    "\n",
    "        try:\n",
    "            crawler = BFSCrawler(seed_url, checkpoint_id)\n",
    "            companies, stats, graph = crawler.crawl()\n",
    "\n",
    "            all_companies.extend(companies)\n",
    "            all_stats.append(stats)\n",
    "            all_graphs.append((seed_url, graph))\n",
    "\n",
    "            # Save incremental results\n",
    "            if companies:\n",
    "                temp_df = pd.DataFrame([asdict(c) for c in companies])\n",
    "                temp_file = Config.OUTPUT_DIR / f\"temp_{checkpoint_id}.csv\"\n",
    "                temp_df.to_csv(temp_file, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {seed_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Compile final results\n",
    "    if not all_companies:\n",
    "        print(\"\\n‚ùå No companies extracted\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä GENERATING FINAL RESULTS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    final_df = pd.DataFrame([asdict(c) for c in all_companies])\n",
    "\n",
    "    # Deduplicate\n",
    "    print(f\"Before deduplication: {len(final_df)} records\")\n",
    "    final_df = final_df.drop_duplicates(subset=['name', 'phone', 'website'], keep='first')\n",
    "    print(f\"After deduplication: {len(final_df)} records\")\n",
    "\n",
    "    # Save final CSV\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = Config.OUTPUT_DIR / f\"zimbabwe_companies_{timestamp}.csv\"\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n‚úì Final dataset saved: {output_file}\")\n",
    "\n",
    "    # Generate analytics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìà GENERATING ANALYTICS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Summary statistics\n",
    "    summary_df = Analytics.generate_summary(all_stats)\n",
    "    summary_file = Config.STATS_DIR / f\"summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"‚úì Summary statistics: {summary_file}\")\n",
    "    print(f\"\\n{summary_df.to_string()}\\n\")\n",
    "\n",
    "    # Performance plots\n",
    "    perf_plot = Config.STATS_DIR / f\"performance_{timestamp}.png\"\n",
    "    Analytics.plot_performance(all_stats, perf_plot)\n",
    "\n",
    "    # Data quality plots\n",
    "    quality_plot = Config.STATS_DIR / f\"data_quality_{timestamp}.png\"\n",
    "    Analytics.plot_data_quality(final_df, quality_plot)\n",
    "\n",
    "    # Network graphs (first 5 seeds)\n",
    "    for i, (seed_url, graph) in enumerate(all_graphs[:5], 1):\n",
    "        network_file = Config.STATS_DIR / f\"network_seed{i}_{timestamp}.png\"\n",
    "        Analytics.plot_network(graph, seed_url, network_file)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ EXTRACTION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total companies: {len(final_df)}\")\n",
    "    print(f\"Total pages crawled: {sum(s.pages_crawled for s in all_stats)}\")\n",
    "    print(f\"Total duration: {sum(s.duration() for s in all_stats)/60:.1f} minutes\")\n",
    "    print(f\"Average speed: {sum(s.pages_per_minute() for s in all_stats)/len(all_stats):.1f} pages/min\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# ============================================\n",
    "# EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load input data\n",
    "    try:\n",
    "        directories_with_listings\n",
    "    except NameError:\n",
    "        try:\n",
    "            directories_with_listings = pd.read_csv(\"directories_with_listings.csv\")\n",
    "            print(\"‚úì Loaded directories_with_listings.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            print(\"Please load 'directories_with_listings' DataFrame\")\n",
    "            raise\n",
    "\n",
    "    # Run extraction\n",
    "    import numpy as np  # For trendline\n",
    "    companies_df = extract_all_companies(directories_with_listings, max_workers=2)\n",
    "\n",
    "    # Display sample\n",
    "    if not companies_df.empty:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SAMPLE RESULTS (First 10 records)\")\n",
    "        print(\"=\"*70)\n",
    "        print(companies_df.head(10).to_string())\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FIELD STATISTICS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Name:        {(companies_df['name'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Address:     {(companies_df['address'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"City:        {(companies_df['city'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Phone:       {(companies_df['phone'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Email:       {(companies_df['email'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Website:     {(companies_df['website'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(f\"Description: {(companies_df['description'] != '').sum():>6} / {len(companies_df)}\")\n",
    "        print(\"=\"*70 + \"\\n\")"
   ],
   "id": "abf0d590caf57ea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded directories_with_listings.csv\n",
      "\n",
      "======================================================================\n",
      "üöÄ ZIMBABWE COMPANY EXTRACTOR\n",
      "======================================================================\n",
      "Total seed URLs: 7\n",
      "Parallel workers: 2\n",
      "Checkpoint directory: checkpoints\n",
      "Output directory: output\n",
      "======================================================================\n",
      "\n",
      "\n",
      "[1/7] Processing: https://www.zimbabweyp.com/browse-business-directory\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://www.zimbabweyp.com/browse-business-directory\n",
      "Domain: zimbabweyp.com\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 202 companies | Queue: 389 | Depth: 1\n",
      "  Progress: 20 pages | 602 companies | Queue: 389 | Depth: 1\n",
      "  Progress: 30 pages | 952 companies | Queue: 385 | Depth: 1\n",
      "  Progress: 40 pages | 1040 companies | Queue: 375 | Depth: 1\n",
      "  Progress: 50 pages | 1098 companies | Queue: 366 | Depth: 1\n",
      "  Progress: 60 pages | 1498 companies | Queue: 366 | Depth: 1\n",
      "  Progress: 70 pages | 1790 companies | Queue: 362 | Depth: 1\n",
      "  Progress: 80 pages | 1840 companies | Queue: 352 | Depth: 1\n",
      "  Progress: 90 pages | 2012 companies | Queue: 346 | Depth: 1\n",
      "  Progress: 100 pages | 2204 companies | Queue: 339 | Depth: 1\n",
      "  Progress: 110 pages | 2486 companies | Queue: 336 | Depth: 1\n",
      "  Progress: 120 pages | 2736 companies | Queue: 329 | Depth: 1\n",
      "  Progress: 130 pages | 2884 companies | Queue: 322 | Depth: 1\n",
      "  Progress: 140 pages | 3092 companies | Queue: 315 | Depth: 1\n",
      "  Progress: 150 pages | 3452 companies | Queue: 313 | Depth: 1\n",
      "  Progress: 160 pages | 3598 companies | Queue: 303 | Depth: 1\n",
      "  Progress: 170 pages | 3762 companies | Queue: 295 | Depth: 1\n",
      "  Progress: 180 pages | 4162 companies | Queue: 295 | Depth: 1\n",
      "  Progress: 190 pages | 4496 companies | Queue: 290 | Depth: 1\n",
      "  Progress: 200 pages | 4678 companies | Queue: 283 | Depth: 1\n",
      "  Progress: 210 pages | 5076 companies | Queue: 282 | Depth: 1\n",
      "  Progress: 220 pages | 5158 companies | Queue: 271 | Depth: 1\n",
      "  Progress: 230 pages | 5332 companies | Queue: 265 | Depth: 1\n",
      "  Progress: 240 pages | 5600 companies | Queue: 256 | Depth: 1\n",
      "  Progress: 250 pages | 5714 companies | Queue: 247 | Depth: 1\n",
      "  Progress: 260 pages | 6114 companies | Queue: 247 | Depth: 1\n",
      "  Progress: 270 pages | 6400 companies | Queue: 239 | Depth: 1\n",
      "  Progress: 280 pages | 6500 companies | Queue: 229 | Depth: 1\n",
      "  Progress: 290 pages | 6716 companies | Queue: 224 | Depth: 1\n",
      "  Progress: 300 pages | 7094 companies | Queue: 222 | Depth: 1\n",
      "  Progress: 310 pages | 7304 companies | Queue: 212 | Depth: 1\n",
      "  Progress: 320 pages | 7400 companies | Queue: 202 | Depth: 1\n",
      "  Progress: 330 pages | 7684 companies | Queue: 199 | Depth: 1\n",
      "  Progress: 340 pages | 7880 companies | Queue: 189 | Depth: 1\n",
      "  Progress: 350 pages | 7964 companies | Queue: 179 | Depth: 1\n",
      "  Progress: 360 pages | 8110 companies | Queue: 172 | Depth: 1\n",
      "  Progress: 370 pages | 8488 companies | Queue: 169 | Depth: 1\n",
      "  Progress: 380 pages | 8664 companies | Queue: 159 | Depth: 1\n",
      "  Progress: 390 pages | 8670 companies | Queue: 148 | Depth: 1\n",
      "  Progress: 400 pages | 9030 companies | Queue: 147 | Depth: 1\n",
      "  Progress: 410 pages | 9390 companies | Queue: 145 | Depth: 1\n",
      "  Progress: 420 pages | 9722 companies | Queue: 143 | Depth: 1\n",
      "  Progress: 430 pages | 10084 companies | Queue: 140 | Depth: 1\n",
      "  Progress: 440 pages | 10434 companies | Queue: 137 | Depth: 1\n",
      "  Progress: 450 pages | 10778 companies | Queue: 133 | Depth: 1\n",
      "  Progress: 460 pages | 11034 companies | Queue: 127 | Depth: 1\n",
      "  Progress: 470 pages | 11434 companies | Queue: 127 | Depth: 1\n",
      "  Progress: 480 pages | 11768 companies | Queue: 123 | Depth: 1\n",
      "  Progress: 490 pages | 12052 companies | Queue: 118 | Depth: 1\n",
      "  Progress: 500 pages | 12410 companies | Queue: 116 | Depth: 1\n",
      "  Progress: 510 pages | 12680 companies | Queue: 111 | Depth: 1\n",
      "  Progress: 520 pages | 12974 companies | Queue: 106 | Depth: 1\n",
      "  Progress: 530 pages | 13278 companies | Queue: 103 | Depth: 1\n",
      "  Progress: 540 pages | 13616 companies | Queue: 98 | Depth: 1\n",
      "  Progress: 550 pages | 14016 companies | Queue: 98 | Depth: 1\n",
      "  Progress: 560 pages | 14360 companies | Queue: 95 | Depth: 1\n",
      "  Progress: 570 pages | 14706 companies | Queue: 92 | Depth: 1\n",
      "  Progress: 580 pages | 15092 companies | Queue: 91 | Depth: 1\n",
      "  Progress: 590 pages | 15446 companies | Queue: 89 | Depth: 1\n",
      "  Progress: 600 pages | 15826 companies | Queue: 87 | Depth: 1\n",
      "  Progress: 610 pages | 16154 companies | Queue: 84 | Depth: 1\n",
      "  Progress: 620 pages | 16468 companies | Queue: 80 | Depth: 1\n",
      "  Progress: 630 pages | 16740 companies | Queue: 76 | Depth: 1\n",
      "  Progress: 640 pages | 17088 companies | Queue: 74 | Depth: 1\n",
      "  Progress: 650 pages | 17488 companies | Queue: 73 | Depth: 1\n",
      "  Progress: 660 pages | 17864 companies | Queue: 72 | Depth: 1\n",
      "  Progress: 670 pages | 18190 companies | Queue: 68 | Depth: 1\n",
      "  Progress: 680 pages | 18572 companies | Queue: 66 | Depth: 1\n",
      "  Progress: 690 pages | 18916 companies | Queue: 62 | Depth: 1\n",
      "  Progress: 700 pages | 19290 companies | Queue: 61 | Depth: 1\n",
      "  Progress: 710 pages | 19646 companies | Queue: 59 | Depth: 1\n",
      "  Progress: 720 pages | 20046 companies | Queue: 59 | Depth: 1\n",
      "  Progress: 730 pages | 20400 companies | Queue: 56 | Depth: 1\n",
      "  Progress: 740 pages | 20686 companies | Queue: 51 | Depth: 1\n",
      "  Progress: 750 pages | 21064 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 760 pages | 21464 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 770 pages | 21792 companies | Queue: 48 | Depth: 1\n",
      "  Progress: 780 pages | 22148 companies | Queue: 45 | Depth: 1\n",
      "  Progress: 790 pages | 22486 companies | Queue: 41 | Depth: 1\n",
      "  Progress: 800 pages | 22848 companies | Queue: 37 | Depth: 1\n",
      "  Progress: 810 pages | 23214 companies | Queue: 35 | Depth: 1\n",
      "  Progress: 820 pages | 23578 companies | Queue: 33 | Depth: 1\n",
      "  Progress: 830 pages | 23946 companies | Queue: 30 | Depth: 1\n",
      "  Progress: 840 pages | 24316 companies | Queue: 29 | Depth: 1\n",
      "  Progress: 850 pages | 24716 companies | Queue: 29 | Depth: 1\n",
      "  Progress: 860 pages | 25074 companies | Queue: 27 | Depth: 1\n",
      "  Progress: 870 pages | 25396 companies | Queue: 24 | Depth: 1\n",
      "  Progress: 880 pages | 25756 companies | Queue: 22 | Depth: 1\n",
      "  Progress: 890 pages | 26122 companies | Queue: 20 | Depth: 1\n",
      "  Progress: 900 pages | 26500 companies | Queue: 17 | Depth: 1\n",
      "  Progress: 910 pages | 26880 companies | Queue: 16 | Depth: 1\n",
      "  Progress: 920 pages | 27220 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 930 pages | 27620 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 940 pages | 27996 companies | Queue: 13 | Depth: 1\n",
      "  Progress: 950 pages | 28334 companies | Queue: 9 | Depth: 1\n",
      "  Progress: 960 pages | 28720 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 970 pages | 29088 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 980 pages | 29480 companies | Queue: 4 | Depth: 1\n",
      "  Progress: 990 pages | 29866 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1000 pages | 30266 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1010 pages | 30666 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1020 pages | 31066 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1030 pages | 31466 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1040 pages | 31860 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1050 pages | 32238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1060 pages | 32638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1070 pages | 33038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1080 pages | 33438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1090 pages | 33838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1100 pages | 34238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1110 pages | 34638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1120 pages | 35038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1130 pages | 35438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1140 pages | 35838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1150 pages | 36238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1160 pages | 36638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1170 pages | 37038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1180 pages | 37438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1190 pages | 37838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1200 pages | 38238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1210 pages | 38638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1220 pages | 39038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1230 pages | 39438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1240 pages | 39838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1250 pages | 40238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1260 pages | 40638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1270 pages | 41038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1280 pages | 41438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1290 pages | 41838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1300 pages | 42238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1310 pages | 42638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1320 pages | 43038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1330 pages | 43438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1340 pages | 43838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1350 pages | 44238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1360 pages | 44638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1370 pages | 45038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1380 pages | 45438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1390 pages | 45838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1400 pages | 46238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1410 pages | 46638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1420 pages | 47038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1430 pages | 47438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1440 pages | 47838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1450 pages | 48238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1460 pages | 48638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1470 pages | 49038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1480 pages | 49438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1490 pages | 49838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1500 pages | 50238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1510 pages | 50638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1520 pages | 51038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1530 pages | 51438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1540 pages | 51838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1550 pages | 52238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1560 pages | 52638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1570 pages | 53038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1580 pages | 53438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1590 pages | 53838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1600 pages | 54238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1610 pages | 54638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1620 pages | 55038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1630 pages | 55438 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1640 pages | 55838 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1650 pages | 56238 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1660 pages | 56638 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1670 pages | 57038 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1680 pages | 57438 companies | Queue: 1 | Depth: 1\n",
      "\n",
      "Fetching 22310 detail pages...\n",
      "  Detail pages: 50/2000 (57790 companies)\n",
      "  Detail pages: 100/2000 (57840 companies)\n",
      "  Detail pages: 150/2000 (57890 companies)\n",
      "  Detail pages: 200/2000 (57940 companies)\n",
      "  Detail pages: 250/2000 (57990 companies)\n",
      "  Detail pages: 300/2000 (58040 companies)\n",
      "  Detail pages: 350/2000 (58090 companies)\n",
      "  Detail pages: 400/2000 (58140 companies)\n",
      "  Detail pages: 450/2000 (58190 companies)\n",
      "  Detail pages: 500/2000 (58240 companies)\n",
      "  Detail pages: 550/2000 (58290 companies)\n",
      "  Detail pages: 600/2000 (58340 companies)\n",
      "  Detail pages: 650/2000 (58390 companies)\n",
      "  Detail pages: 700/2000 (58440 companies)\n",
      "  Detail pages: 750/2000 (58490 companies)\n",
      "  Detail pages: 800/2000 (58540 companies)\n",
      "  Detail pages: 850/2000 (58590 companies)\n",
      "  Detail pages: 900/2000 (58640 companies)\n",
      "  Detail pages: 950/2000 (58690 companies)\n",
      "  Detail pages: 1000/2000 (58740 companies)\n",
      "  Detail pages: 1050/2000 (58790 companies)\n",
      "  Detail pages: 1100/2000 (58840 companies)\n",
      "  Detail pages: 1150/2000 (58890 companies)\n",
      "  Detail pages: 1200/2000 (58940 companies)\n",
      "  Detail pages: 1250/2000 (58990 companies)\n",
      "  Detail pages: 1300/2000 (59040 companies)\n",
      "  Detail pages: 1350/2000 (59090 companies)\n",
      "  Detail pages: 1400/2000 (59140 companies)\n",
      "  Detail pages: 1450/2000 (59190 companies)\n",
      "  Detail pages: 1500/2000 (59240 companies)\n",
      "  Detail pages: 1550/2000 (59290 companies)\n",
      "  Detail pages: 1600/2000 (59340 companies)\n",
      "  Detail pages: 1650/2000 (59390 companies)\n",
      "  Detail pages: 1700/2000 (59440 companies)\n",
      "  Detail pages: 1750/2000 (59490 companies)\n",
      "  Detail pages: 1800/2000 (59540 companies)\n",
      "  Detail pages: 1850/2000 (59590 companies)\n",
      "  Detail pages: 1900/2000 (59640 companies)\n",
      "  Detail pages: 1950/2000 (59690 companies)\n",
      "  Detail pages: 2000/2000 (59740 companies)\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 1688\n",
      "  Companies: 59740\n",
      "  Duration: 36.7 min\n",
      "  Speed: 46.0 pages/min\n",
      "\n",
      "[2/7] Processing: https://afrikta.com/listing-locations/zimbabwe/\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://afrikta.com/listing-locations/zimbabwe/\n",
      "Domain: afrikta.com\n",
      "============================================================\n",
      "\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 1\n",
      "  Companies: 0\n",
      "  Duration: 0.0 min\n",
      "  Speed: 22.5 pages/min\n",
      "\n",
      "[3/7] Processing: https://sur.ly/i/yellowpages.co.zw/\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://sur.ly/i/yellowpages.co.zw/\n",
      "Domain: sur.ly\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 0 companies | Queue: 0 | Depth: 1\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 10\n",
      "  Companies: 0\n",
      "  Duration: 0.4 min\n",
      "  Speed: 28.3 pages/min\n",
      "\n",
      "[4/7] Processing: https://thedirectory.co.zw/\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://thedirectory.co.zw/\n",
      "Domain: thedirectory.co.zw\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 46 companies | Queue: 424 | Depth: 1\n",
      "  Progress: 20 pages | 52 companies | Queue: 421 | Depth: 1\n",
      "  Progress: 30 pages | 121 companies | Queue: 420 | Depth: 1\n",
      "  Progress: 40 pages | 169 companies | Queue: 418 | Depth: 1\n",
      "  Progress: 50 pages | 257 companies | Queue: 414 | Depth: 1\n",
      "  Progress: 60 pages | 345 companies | Queue: 412 | Depth: 1\n",
      "  Progress: 70 pages | 422 companies | Queue: 411 | Depth: 1\n",
      "  Progress: 80 pages | 468 companies | Queue: 408 | Depth: 1\n",
      "  Progress: 90 pages | 555 companies | Queue: 400 | Depth: 1\n",
      "  Progress: 100 pages | 642 companies | Queue: 395 | Depth: 1\n",
      "  Progress: 110 pages | 810 companies | Queue: 391 | Depth: 1\n",
      "  Progress: 120 pages | 1013 companies | Queue: 383 | Depth: 1\n",
      "  Progress: 130 pages | 1189 companies | Queue: 376 | Depth: 1\n",
      "  Progress: 140 pages | 1198 companies | Queue: 364 | Depth: 1\n",
      "  Progress: 150 pages | 1241 companies | Queue: 360 | Depth: 1\n",
      "  Progress: 160 pages | 1245 companies | Queue: 341 | Depth: 1\n",
      "  Progress: 170 pages | 1473 companies | Queue: 331 | Depth: 1\n",
      "  Progress: 180 pages | 1474 companies | Queue: 318 | Depth: 1\n",
      "  Progress: 190 pages | 1474 companies | Queue: 304 | Depth: 1\n",
      "  Progress: 200 pages | 1474 companies | Queue: 297 | Depth: 1\n",
      "  Progress: 210 pages | 1558 companies | Queue: 285 | Depth: 1\n",
      "  Progress: 220 pages | 1686 companies | Queue: 278 | Depth: 1\n",
      "  Progress: 230 pages | 1826 companies | Queue: 278 | Depth: 1\n",
      "  Progress: 240 pages | 1968 companies | Queue: 278 | Depth: 1\n",
      "  Progress: 250 pages | 2043 companies | Queue: 240 | Depth: 1\n",
      "  Progress: 260 pages | 2052 companies | Queue: 227 | Depth: 1\n",
      "  Progress: 270 pages | 2083 companies | Queue: 126 | Depth: 1\n",
      "\n",
      "Fetching 321 detail pages...\n",
      "  Detail pages: 50/321 (2133 companies)\n",
      "  Detail pages: 100/321 (2183 companies)\n",
      "  Detail pages: 150/321 (2233 companies)\n",
      "  Detail pages: 200/321 (2283 companies)\n",
      "  Detail pages: 250/321 (2333 companies)\n",
      "  Detail pages: 300/321 (2382 companies)\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 273\n",
      "  Companies: 2403\n",
      "  Duration: 7.6 min\n",
      "  Speed: 36.1 pages/min\n",
      "\n",
      "[5/7] Processing: https://www.zimplaza.co.zw/listing/zimyellowpage/\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://www.zimplaza.co.zw/listing/zimyellowpage/\n",
      "Domain: zimplaza.co.zw\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 266 companies | Queue: 225 | Depth: 1\n",
      "  Progress: 20 pages | 612 companies | Queue: 223 | Depth: 1\n",
      "  Progress: 30 pages | 773 companies | Queue: 175 | Depth: 1\n",
      "  Progress: 40 pages | 1000 companies | Queue: 163 | Depth: 1\n",
      "  Progress: 50 pages | 1410 companies | Queue: 161 | Depth: 1\n",
      "  Progress: 60 pages | 1814 companies | Queue: 158 | Depth: 1\n",
      "  Progress: 70 pages | 2212 companies | Queue: 154 | Depth: 1\n",
      "  Progress: 80 pages | 2626 companies | Queue: 151 | Depth: 1\n",
      "  Progress: 90 pages | 2984 companies | Queue: 146 | Depth: 1\n",
      "  Progress: 100 pages | 3306 companies | Queue: 139 | Depth: 1\n",
      "  Progress: 110 pages | 3616 companies | Queue: 132 | Depth: 1\n",
      "  Progress: 120 pages | 4026 companies | Queue: 128 | Depth: 1\n",
      "  Progress: 130 pages | 4434 companies | Queue: 124 | Depth: 1\n",
      "  Progress: 140 pages | 4828 companies | Queue: 121 | Depth: 1\n",
      "  Progress: 150 pages | 5184 companies | Queue: 117 | Depth: 1\n",
      "  Progress: 160 pages | 5526 companies | Queue: 113 | Depth: 1\n",
      "  Progress: 170 pages | 5956 companies | Queue: 111 | Depth: 1\n",
      "  Progress: 180 pages | 6216 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 190 pages | 6498 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 200 pages | 6806 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 210 pages | 7188 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 220 pages | 7532 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 230 pages | 7916 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 240 pages | 8240 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 250 pages | 8588 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 260 pages | 8894 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 270 pages | 9302 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 280 pages | 9732 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 290 pages | 9958 companies | Queue: 109 | Depth: 1\n",
      "  Progress: 300 pages | 10252 companies | Queue: 103 | Depth: 1\n",
      "  Progress: 310 pages | 10600 companies | Queue: 96 | Depth: 1\n",
      "  Progress: 320 pages | 10906 companies | Queue: 90 | Depth: 1\n",
      "  Progress: 330 pages | 11250 companies | Queue: 81 | Depth: 1\n",
      "  Progress: 340 pages | 11650 companies | Queue: 75 | Depth: 1\n",
      "  Progress: 350 pages | 12080 companies | Queue: 74 | Depth: 1\n",
      "  Progress: 360 pages | 12408 companies | Queue: 74 | Depth: 1\n",
      "  Progress: 370 pages | 12582 companies | Queue: 72 | Depth: 1\n",
      "  Progress: 380 pages | 13012 companies | Queue: 68 | Depth: 1\n",
      "  Progress: 390 pages | 13350 companies | Queue: 62 | Depth: 1\n",
      "  Progress: 400 pages | 13748 companies | Queue: 59 | Depth: 1\n",
      "  Progress: 410 pages | 14146 companies | Queue: 59 | Depth: 1\n",
      "  Progress: 420 pages | 14406 companies | Queue: 58 | Depth: 1\n",
      "  Progress: 430 pages | 14630 companies | Queue: 56 | Depth: 1\n",
      "  Progress: 440 pages | 14938 companies | Queue: 53 | Depth: 1\n",
      "  Progress: 450 pages | 15216 companies | Queue: 52 | Depth: 1\n",
      "  Progress: 460 pages | 15616 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 470 pages | 15876 companies | Queue: 50 | Depth: 1\n",
      "  Progress: 480 pages | 16130 companies | Queue: 45 | Depth: 1\n",
      "  Progress: 490 pages | 16536 companies | Queue: 36 | Depth: 1\n",
      "  Progress: 500 pages | 16966 companies | Queue: 36 | Depth: 1\n",
      "  Progress: 510 pages | 17090 companies | Queue: 36 | Depth: 1\n",
      "  Progress: 520 pages | 17510 companies | Queue: 31 | Depth: 1\n",
      "  Progress: 530 pages | 17940 companies | Queue: 31 | Depth: 1\n",
      "  Progress: 540 pages | 18098 companies | Queue: 31 | Depth: 1\n",
      "  Progress: 550 pages | 18470 companies | Queue: 30 | Depth: 1\n",
      "  Progress: 560 pages | 18866 companies | Queue: 30 | Depth: 1\n",
      "  Progress: 570 pages | 19026 companies | Queue: 30 | Depth: 1\n",
      "  Progress: 580 pages | 19422 companies | Queue: 29 | Depth: 1\n",
      "  Progress: 590 pages | 19852 companies | Queue: 28 | Depth: 1\n",
      "  Progress: 600 pages | 19952 companies | Queue: 28 | Depth: 1\n",
      "  Progress: 610 pages | 20382 companies | Queue: 28 | Depth: 1\n",
      "  Progress: 620 pages | 20744 companies | Queue: 28 | Depth: 1\n",
      "  Progress: 630 pages | 20932 companies | Queue: 27 | Depth: 1\n",
      "  Progress: 640 pages | 21342 companies | Queue: 27 | Depth: 1\n",
      "  Progress: 650 pages | 21603 companies | Queue: 26 | Depth: 1\n",
      "  Progress: 660 pages | 21897 companies | Queue: 24 | Depth: 1\n",
      "  Progress: 670 pages | 22327 companies | Queue: 24 | Depth: 1\n",
      "  Progress: 680 pages | 22485 companies | Queue: 24 | Depth: 1\n",
      "  Progress: 690 pages | 22889 companies | Queue: 24 | Depth: 1\n",
      "  Progress: 700 pages | 23116 companies | Queue: 24 | Depth: 1\n",
      "  Progress: 710 pages | 23414 companies | Queue: 22 | Depth: 1\n",
      "  Progress: 720 pages | 23708 companies | Queue: 22 | Depth: 1\n",
      "  Progress: 730 pages | 23968 companies | Queue: 21 | Depth: 1\n",
      "  Progress: 740 pages | 24261 companies | Queue: 21 | Depth: 1\n",
      "  Progress: 750 pages | 24487 companies | Queue: 21 | Depth: 1\n",
      "  Progress: 760 pages | 24815 companies | Queue: 20 | Depth: 1\n",
      "  Progress: 770 pages | 25041 companies | Queue: 20 | Depth: 1\n",
      "  Progress: 780 pages | 25337 companies | Queue: 20 | Depth: 1\n",
      "  Progress: 790 pages | 25563 companies | Queue: 20 | Depth: 1\n",
      "  Progress: 800 pages | 25857 companies | Queue: 19 | Depth: 1\n",
      "  Progress: 810 pages | 26105 companies | Queue: 19 | Depth: 1\n",
      "  Progress: 820 pages | 26365 companies | Queue: 19 | Depth: 1\n",
      "  Progress: 830 pages | 26659 companies | Queue: 17 | Depth: 1\n",
      "  Progress: 840 pages | 26818 companies | Queue: 17 | Depth: 1\n",
      "  Progress: 850 pages | 27180 companies | Queue: 17 | Depth: 1\n",
      "  Progress: 860 pages | 27312 companies | Queue: 17 | Depth: 1\n",
      "  Progress: 870 pages | 27594 companies | Queue: 17 | Depth: 1\n",
      "  Progress: 880 pages | 27854 companies | Queue: 16 | Depth: 1\n",
      "  Progress: 890 pages | 27978 companies | Queue: 15 | Depth: 1\n",
      "  Progress: 900 pages | 28272 companies | Queue: 15 | Depth: 1\n",
      "  Progress: 910 pages | 28532 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 920 pages | 28654 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 930 pages | 28881 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 940 pages | 29141 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 950 pages | 29334 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 960 pages | 29492 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 970 pages | 29753 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 980 pages | 29975 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 990 pages | 30099 companies | Queue: 14 | Depth: 1\n",
      "  Progress: 1000 pages | 30291 companies | Queue: 13 | Depth: 1\n",
      "  Progress: 1010 pages | 30517 companies | Queue: 12 | Depth: 1\n",
      "  Progress: 1020 pages | 30709 companies | Queue: 12 | Depth: 1\n",
      "  Progress: 1030 pages | 30899 companies | Queue: 12 | Depth: 1\n",
      "  Progress: 1040 pages | 31023 companies | Queue: 12 | Depth: 1\n",
      "  Progress: 1050 pages | 31147 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1060 pages | 31305 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1070 pages | 31463 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1080 pages | 31621 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1090 pages | 31779 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1100 pages | 31937 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1110 pages | 32095 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1120 pages | 32253 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1130 pages | 32411 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1140 pages | 32569 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1150 pages | 32693 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1160 pages | 32817 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1170 pages | 32975 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1180 pages | 33133 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1190 pages | 33291 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1200 pages | 33449 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1210 pages | 33608 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1220 pages | 33767 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1230 pages | 33926 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1240 pages | 34084 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1250 pages | 34242 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1260 pages | 34366 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1270 pages | 34490 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1280 pages | 34648 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1290 pages | 34806 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1300 pages | 34964 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1310 pages | 35122 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1320 pages | 35280 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1330 pages | 35438 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1340 pages | 35596 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1350 pages | 35754 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1360 pages | 35912 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1370 pages | 36036 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1380 pages | 36160 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1390 pages | 36319 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1400 pages | 36477 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1410 pages | 36635 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1420 pages | 36793 companies | Queue: 11 | Depth: 1\n",
      "  Progress: 1430 pages | 36986 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1440 pages | 37212 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1450 pages | 37438 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1460 pages | 37664 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1470 pages | 37890 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1480 pages | 38116 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1490 pages | 38342 companies | Queue: 5 | Depth: 1\n",
      "  Progress: 1500 pages | 38534 companies | Queue: 4 | Depth: 1\n",
      "  Progress: 1510 pages | 38692 companies | Queue: 4 | Depth: 1\n",
      "  Progress: 1520 pages | 38884 companies | Queue: 4 | Depth: 1\n",
      "  Progress: 1530 pages | 39042 companies | Queue: 4 | Depth: 1\n",
      "  Progress: 1540 pages | 39234 companies | Queue: 4 | Depth: 1\n",
      "  Progress: 1550 pages | 39426 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1560 pages | 39618 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1570 pages | 39846 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1580 pages | 40038 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1590 pages | 40231 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1600 pages | 40457 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1610 pages | 40649 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1620 pages | 40841 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1630 pages | 41067 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1640 pages | 41259 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1650 pages | 41451 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1660 pages | 41677 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1670 pages | 41869 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1680 pages | 42061 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1690 pages | 42287 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1700 pages | 42479 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1710 pages | 42671 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1720 pages | 42897 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1730 pages | 43089 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1740 pages | 43281 companies | Queue: 3 | Depth: 1\n",
      "  Progress: 1750 pages | 43507 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1760 pages | 43767 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1770 pages | 44027 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1780 pages | 44287 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1790 pages | 44547 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1800 pages | 44807 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1810 pages | 45067 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1820 pages | 45327 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1830 pages | 45587 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1840 pages | 45847 companies | Queue: 2 | Depth: 1\n",
      "  Progress: 1850 pages | 46039 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1860 pages | 46129 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1870 pages | 46219 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1880 pages | 46309 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1890 pages | 46399 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1900 pages | 46489 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1910 pages | 46579 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1920 pages | 46669 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1930 pages | 46759 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1940 pages | 46853 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1950 pages | 46952 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1960 pages | 47045 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1970 pages | 47135 companies | Queue: 1 | Depth: 1\n",
      "  Progress: 1980 pages | 47225 companies | Queue: 1 | Depth: 1\n",
      "\n",
      "Fetching 8685 detail pages...\n",
      "  Detail pages: 50/2000 (47331 companies)\n",
      "  Detail pages: 100/2000 (47381 companies)\n",
      "  Detail pages: 150/2000 (47431 companies)\n",
      "  Detail pages: 200/2000 (47481 companies)\n",
      "  Detail pages: 250/2000 (47531 companies)\n",
      "  Detail pages: 300/2000 (47581 companies)\n",
      "  Detail pages: 350/2000 (47631 companies)\n",
      "  Detail pages: 400/2000 (47681 companies)\n",
      "  Detail pages: 450/2000 (47731 companies)\n",
      "  Detail pages: 500/2000 (47781 companies)\n",
      "  Detail pages: 550/2000 (47831 companies)\n",
      "  Detail pages: 600/2000 (47881 companies)\n",
      "  Detail pages: 650/2000 (47931 companies)\n",
      "  Detail pages: 700/2000 (47981 companies)\n",
      "  Detail pages: 750/2000 (48031 companies)\n",
      "  Detail pages: 800/2000 (48081 companies)\n",
      "  Detail pages: 850/2000 (48131 companies)\n",
      "  Detail pages: 900/2000 (48181 companies)\n",
      "  Detail pages: 950/2000 (48231 companies)\n",
      "  Detail pages: 1000/2000 (48281 companies)\n",
      "  Detail pages: 1050/2000 (48331 companies)\n",
      "  Detail pages: 1100/2000 (48381 companies)\n",
      "  Detail pages: 1150/2000 (48431 companies)\n",
      "  Detail pages: 1200/2000 (48481 companies)\n",
      "  Detail pages: 1250/2000 (48531 companies)\n",
      "  Detail pages: 1300/2000 (48581 companies)\n",
      "  Detail pages: 1350/2000 (48631 companies)\n",
      "  Detail pages: 1400/2000 (48681 companies)\n",
      "  Detail pages: 1450/2000 (48731 companies)\n",
      "  Detail pages: 1500/2000 (48781 companies)\n",
      "  Detail pages: 1550/2000 (48831 companies)\n",
      "  Detail pages: 1600/2000 (48881 companies)\n",
      "  Detail pages: 1650/2000 (48931 companies)\n",
      "  Detail pages: 1700/2000 (48980 companies)\n",
      "  Detail pages: 1750/2000 (49030 companies)\n",
      "  Detail pages: 1800/2000 (49080 companies)\n",
      "  Detail pages: 1850/2000 (49129 companies)\n",
      "  Detail pages: 1900/2000 (49179 companies)\n",
      "  Detail pages: 1950/2000 (49229 companies)\n",
      "  Detail pages: 2000/2000 (49279 companies)\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 1987\n",
      "  Companies: 49279\n",
      "  Duration: 195.3 min\n",
      "  Speed: 10.2 pages/min\n",
      "\n",
      "[6/7] Processing: https://www.zimyellow.com/\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://www.zimyellow.com/\n",
      "Domain: zimyellow.com\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 20 companies | Queue: 57 | Depth: 1\n",
      "  Progress: 20 pages | 47 companies | Queue: 35 | Depth: 1\n",
      "  Progress: 30 pages | 70 companies | Queue: 13 | Depth: 1\n",
      "  Progress: 40 pages | 97 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 50 pages | 118 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 60 pages | 140 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 70 pages | 161 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 80 pages | 182 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 90 pages | 204 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 100 pages | 225 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 110 pages | 247 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 120 pages | 268 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 130 pages | 290 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 140 pages | 311 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 150 pages | 332 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 160 pages | 354 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 170 pages | 375 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 180 pages | 397 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 190 pages | 418 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 200 pages | 440 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 210 pages | 461 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 220 pages | 482 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 230 pages | 504 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 240 pages | 525 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 250 pages | 547 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 260 pages | 568 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 270 pages | 590 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 280 pages | 611 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 290 pages | 632 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 300 pages | 654 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 310 pages | 675 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 320 pages | 697 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 330 pages | 718 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 340 pages | 740 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 350 pages | 761 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 360 pages | 782 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 370 pages | 804 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 380 pages | 825 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 390 pages | 847 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 400 pages | 868 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 410 pages | 890 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 420 pages | 911 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 430 pages | 932 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 440 pages | 954 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 450 pages | 975 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 460 pages | 997 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 470 pages | 1018 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 480 pages | 1040 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 490 pages | 1061 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 500 pages | 1082 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 510 pages | 1104 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 520 pages | 1125 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 530 pages | 1147 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 540 pages | 1168 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 550 pages | 1190 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 560 pages | 1211 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 570 pages | 1232 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 580 pages | 1254 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 590 pages | 1275 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 600 pages | 1297 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 610 pages | 1318 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 620 pages | 1340 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 630 pages | 1361 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 640 pages | 1382 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 650 pages | 1404 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 660 pages | 1425 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 670 pages | 1447 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 680 pages | 1468 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 690 pages | 1490 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 700 pages | 1511 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 710 pages | 1532 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 720 pages | 1554 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 730 pages | 1575 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 740 pages | 1597 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 750 pages | 1618 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 760 pages | 1640 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 770 pages | 1661 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 780 pages | 1682 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 790 pages | 1704 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 800 pages | 1725 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 810 pages | 1747 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 820 pages | 1768 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 830 pages | 1790 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 840 pages | 1811 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 850 pages | 1832 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 860 pages | 1854 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 870 pages | 1875 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 880 pages | 1897 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 890 pages | 1918 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 900 pages | 1940 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 910 pages | 1961 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 920 pages | 1982 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 930 pages | 2004 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 940 pages | 2025 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 950 pages | 2047 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 960 pages | 2068 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 970 pages | 2090 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 980 pages | 2111 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 990 pages | 2132 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1000 pages | 2154 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1010 pages | 2175 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1020 pages | 2197 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1030 pages | 2218 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1040 pages | 2240 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1050 pages | 2261 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1060 pages | 2282 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1070 pages | 2304 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1080 pages | 2325 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1090 pages | 2347 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1100 pages | 2368 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1110 pages | 2390 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1120 pages | 2411 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1130 pages | 2432 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1140 pages | 2454 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1150 pages | 2475 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1160 pages | 2497 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1170 pages | 2518 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1180 pages | 2540 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1190 pages | 2561 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1200 pages | 2582 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1210 pages | 2604 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1220 pages | 2625 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1230 pages | 2647 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1240 pages | 2668 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1250 pages | 2690 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1260 pages | 2711 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1270 pages | 2732 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1280 pages | 2754 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1290 pages | 2775 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1300 pages | 2797 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1310 pages | 2818 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1320 pages | 2840 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1330 pages | 2861 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1340 pages | 2882 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1350 pages | 2904 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1360 pages | 2925 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1370 pages | 2947 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1380 pages | 2968 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1390 pages | 2990 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1400 pages | 3011 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1410 pages | 3032 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1420 pages | 3054 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1430 pages | 3075 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1440 pages | 3097 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1450 pages | 3118 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1460 pages | 3140 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1470 pages | 3161 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1480 pages | 3182 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1490 pages | 3204 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1500 pages | 3225 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1510 pages | 3247 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1520 pages | 3268 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1530 pages | 3290 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1540 pages | 3311 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1550 pages | 3332 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1560 pages | 3354 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1570 pages | 3375 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1580 pages | 3397 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1590 pages | 3418 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1600 pages | 3440 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1610 pages | 3461 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1620 pages | 3482 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1630 pages | 3504 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1640 pages | 3525 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1650 pages | 3547 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1660 pages | 3568 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1670 pages | 3590 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1680 pages | 3611 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1690 pages | 3632 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1700 pages | 3654 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1710 pages | 3675 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1720 pages | 3697 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1730 pages | 3718 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1740 pages | 3740 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1750 pages | 3761 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1760 pages | 3782 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1770 pages | 3804 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1780 pages | 3825 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1790 pages | 3847 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1800 pages | 3868 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1810 pages | 3890 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1820 pages | 3911 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1830 pages | 3932 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1840 pages | 3954 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1850 pages | 3975 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1860 pages | 3997 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1870 pages | 4018 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1880 pages | 4040 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1890 pages | 4061 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1900 pages | 4082 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1910 pages | 4104 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1920 pages | 4125 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1930 pages | 4147 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1940 pages | 4168 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1950 pages | 4190 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1960 pages | 4211 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1970 pages | 4232 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1980 pages | 4254 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 1990 pages | 4275 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2000 pages | 4297 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2010 pages | 4318 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2020 pages | 4340 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2030 pages | 4361 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2040 pages | 4382 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2050 pages | 4404 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2060 pages | 4425 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2070 pages | 4447 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2080 pages | 4468 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2090 pages | 4490 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2100 pages | 4511 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2110 pages | 4532 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2120 pages | 4554 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2130 pages | 4575 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2140 pages | 4597 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2150 pages | 4618 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2160 pages | 4640 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2170 pages | 4661 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2180 pages | 4682 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2190 pages | 4704 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2200 pages | 4725 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2210 pages | 4747 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2220 pages | 4768 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2230 pages | 4790 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2240 pages | 4811 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2250 pages | 4832 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2260 pages | 4854 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2270 pages | 4875 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2280 pages | 4897 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2290 pages | 4918 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2300 pages | 4940 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2310 pages | 4961 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2320 pages | 4982 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2330 pages | 5004 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2340 pages | 5025 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2350 pages | 5047 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2360 pages | 5068 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2370 pages | 5090 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2380 pages | 5111 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2390 pages | 5132 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2400 pages | 5154 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2410 pages | 5175 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2420 pages | 5197 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2430 pages | 5218 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2440 pages | 5240 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2450 pages | 5261 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2460 pages | 5282 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2470 pages | 5304 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2480 pages | 5325 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2490 pages | 5347 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2500 pages | 5368 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2510 pages | 5390 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2520 pages | 5411 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2530 pages | 5432 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2540 pages | 5454 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2550 pages | 5475 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2560 pages | 5497 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2570 pages | 5518 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2580 pages | 5540 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2590 pages | 5561 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2600 pages | 5582 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2610 pages | 5604 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2620 pages | 5625 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2630 pages | 5647 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2640 pages | 5668 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2650 pages | 5690 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2660 pages | 5711 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2670 pages | 5732 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2680 pages | 5754 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2690 pages | 5775 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2700 pages | 5797 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2710 pages | 5818 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2720 pages | 5840 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2730 pages | 5861 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2740 pages | 5882 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2750 pages | 5904 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2760 pages | 5925 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2770 pages | 5947 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2780 pages | 5968 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2790 pages | 5990 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2800 pages | 6011 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2810 pages | 6032 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2820 pages | 6054 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2830 pages | 6075 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2840 pages | 6097 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2850 pages | 6118 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2860 pages | 6140 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2870 pages | 6161 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2880 pages | 6182 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2890 pages | 6204 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2900 pages | 6225 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2910 pages | 6247 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2920 pages | 6268 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2930 pages | 6290 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2940 pages | 6311 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2950 pages | 6332 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2960 pages | 6354 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2970 pages | 6375 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2980 pages | 6397 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 2990 pages | 6418 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3000 pages | 6440 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3010 pages | 6461 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3020 pages | 6482 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3030 pages | 6504 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3040 pages | 6525 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3050 pages | 6547 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3060 pages | 6568 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3070 pages | 6590 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3080 pages | 6611 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3090 pages | 6632 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3100 pages | 6654 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3110 pages | 6675 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3120 pages | 6697 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3130 pages | 6718 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3140 pages | 6740 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3150 pages | 6761 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3160 pages | 6782 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3170 pages | 6804 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3180 pages | 6825 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3190 pages | 6847 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3200 pages | 6868 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3210 pages | 6890 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3220 pages | 6911 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3230 pages | 6932 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3240 pages | 6954 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3250 pages | 6975 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3260 pages | 6997 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3270 pages | 7018 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3280 pages | 7040 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3290 pages | 7061 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3300 pages | 7082 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3310 pages | 7104 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3320 pages | 7125 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3330 pages | 7147 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3340 pages | 7168 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3350 pages | 7190 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3360 pages | 7211 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3370 pages | 7232 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3380 pages | 7254 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3390 pages | 7275 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3400 pages | 7297 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3410 pages | 7318 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3420 pages | 7340 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3430 pages | 7361 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3440 pages | 7382 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3450 pages | 7404 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3460 pages | 7425 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3470 pages | 7447 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3480 pages | 7468 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3490 pages | 7490 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3500 pages | 7511 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3510 pages | 7532 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3520 pages | 7554 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3530 pages | 7575 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3540 pages | 7597 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3550 pages | 7618 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3560 pages | 7640 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3570 pages | 7661 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3580 pages | 7682 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3590 pages | 7704 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3600 pages | 7725 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3610 pages | 7747 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3620 pages | 7768 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3630 pages | 7790 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3640 pages | 7811 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3650 pages | 7832 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3660 pages | 7854 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3670 pages | 7875 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3680 pages | 7897 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3690 pages | 7918 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3700 pages | 7940 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3710 pages | 7961 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3720 pages | 7982 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3730 pages | 8004 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3740 pages | 8025 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3750 pages | 8047 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3760 pages | 8068 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3770 pages | 8090 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3780 pages | 8111 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3790 pages | 8132 companies | Queue: 7 | Depth: 1\n",
      "  Progress: 3800 pages | 8154 companies | Queue: 7 | Depth: 1\n",
      "\n",
      "Fetching 542 detail pages...\n",
      "  Detail pages: 50/542 (8204 companies)\n",
      "  Detail pages: 100/542 (8254 companies)\n",
      "  Detail pages: 150/542 (8304 companies)\n",
      "  Detail pages: 200/542 (8354 companies)\n",
      "  Detail pages: 250/542 (8404 companies)\n",
      "  Detail pages: 300/542 (8454 companies)\n",
      "  Detail pages: 350/542 (8504 companies)\n",
      "  Detail pages: 400/542 (8554 companies)\n",
      "  Detail pages: 450/542 (8604 companies)\n",
      "  Detail pages: 500/542 (8654 companies)\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 3800\n",
      "  Companies: 8696\n",
      "  Duration: 121.9 min\n",
      "  Speed: 31.2 pages/min\n",
      "\n",
      "[7/7] Processing: https://thedirectory.co.zw/listings.cfm?SmartPages\n",
      "\n",
      "============================================================\n",
      "Starting BFS crawl: https://thedirectory.co.zw/listings.cfm?SmartPages\n",
      "Domain: thedirectory.co.zw\n",
      "============================================================\n",
      "\n",
      "  Progress: 10 pages | 28 companies | Queue: 427 | Depth: 1\n",
      "  Progress: 20 pages | 35 companies | Queue: 424 | Depth: 1\n",
      "  Progress: 30 pages | 123 companies | Queue: 422 | Depth: 1\n",
      "  Progress: 40 pages | 211 companies | Queue: 418 | Depth: 1\n",
      "  Progress: 50 pages | 260 companies | Queue: 417 | Depth: 1\n",
      "  Progress: 60 pages | 335 companies | Queue: 415 | Depth: 1\n",
      "  Progress: 70 pages | 421 companies | Queue: 406 | Depth: 1\n",
      "  Progress: 80 pages | 469 companies | Queue: 405 | Depth: 1\n",
      "  Progress: 90 pages | 537 companies | Queue: 403 | Depth: 1\n",
      "  Progress: 100 pages | 665 companies | Queue: 400 | Depth: 1\n",
      "  Progress: 110 pages | 829 companies | Queue: 397 | Depth: 1\n",
      "  Progress: 120 pages | 929 companies | Queue: 391 | Depth: 1\n",
      "  Progress: 130 pages | 1071 companies | Queue: 387 | Depth: 1\n",
      "  Progress: 140 pages | 1226 companies | Queue: 385 | Depth: 1\n",
      "  Progress: 150 pages | 1226 companies | Queue: 384 | Depth: 1\n",
      "  Progress: 160 pages | 1226 companies | Queue: 384 | Depth: 1\n",
      "  Progress: 170 pages | 1226 companies | Queue: 383 | Depth: 1\n",
      "  Progress: 180 pages | 1226 companies | Queue: 381 | Depth: 1\n",
      "  Progress: 190 pages | 1226 companies | Queue: 381 | Depth: 1\n",
      "  Progress: 200 pages | 1226 companies | Queue: 379 | Depth: 1\n",
      "  Progress: 210 pages | 1226 companies | Queue: 378 | Depth: 1\n",
      "  Progress: 220 pages | 1226 companies | Queue: 377 | Depth: 1\n",
      "  Progress: 230 pages | 1226 companies | Queue: 377 | Depth: 1\n",
      "  Progress: 240 pages | 1226 companies | Queue: 377 | Depth: 1\n",
      "  Progress: 250 pages | 1226 companies | Queue: 374 | Depth: 1\n",
      "  Progress: 260 pages | 1226 companies | Queue: 374 | Depth: 1\n",
      "  Progress: 270 pages | 1226 companies | Queue: 372 | Depth: 1\n",
      "  Progress: 280 pages | 1281 companies | Queue: 363 | Depth: 1\n",
      "  Progress: 290 pages | 1421 companies | Queue: 353 | Depth: 1\n",
      "  Progress: 300 pages | 1561 companies | Queue: 343 | Depth: 1\n",
      "  Progress: 310 pages | 1703 companies | Queue: 333 | Depth: 1\n",
      "  Progress: 320 pages | 1723 companies | Queue: 286 | Depth: 1\n",
      "  Progress: 330 pages | 1738 companies | Queue: 272 | Depth: 1\n",
      "  Progress: 340 pages | 1761 companies | Queue: 160 | Depth: 1\n",
      "\n",
      "Fetching 283 detail pages...\n",
      "  Detail pages: 50/283 (1810 companies)\n",
      "  Detail pages: 100/283 (1860 companies)\n",
      "  Detail pages: 150/283 (1910 companies)\n",
      "  Detail pages: 200/283 (1960 companies)\n",
      "  Detail pages: 250/283 (2010 companies)\n",
      "\n",
      "‚úì Crawl complete!\n",
      "  Pages: 340\n",
      "  Companies: 2043\n",
      "  Duration: 7.1 min\n",
      "  Speed: 47.8 pages/min\n",
      "\n",
      "======================================================================\n",
      "üìä GENERATING FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "Before deduplication: 122161 records\n",
      "After deduplication: 42347 records\n",
      "\n",
      "‚úì Final dataset saved: output\\zimbabwe_companies_20251204_000743.csv\n",
      "\n",
      "======================================================================\n",
      "üìà GENERATING ANALYTICS\n",
      "======================================================================\n",
      "\n",
      "‚úì Summary statistics: statistics\\summary_20251204_000743.csv\n",
      "\n",
      "                                               Seed URL  Duration (min)  Pages Crawled  Detail Pages  Companies Found  Pages/Min  Avg Response (s)  Success Rate %  Errors\n",
      "0  https://www.zimbabweyp.com/browse-business-directory           36.70           1688          2000            59740      45.99             0.434           100.0       0\n",
      "1       https://afrikta.com/listing-locations/zimbabwe/            0.04              1             0                0      22.53             1.878           100.0       0\n",
      "2                   https://sur.ly/i/yellowpages.co.zw/            0.35             10             0                0      28.26             1.621           100.0       0\n",
      "3                           https://thedirectory.co.zw/            7.56            273           321             2403      36.13             1.001            99.7       1\n",
      "4     https://www.zimplaza.co.zw/listing/zimyellowpage/          195.35           1987          2000            49279      10.17             6.144            97.2     105\n",
      "5                            https://www.zimyellow.com/          121.87           3800           542             8696      31.18             1.441           100.0       1\n",
      "6    https://thedirectory.co.zw/listings.cfm?SmartPages            7.11            340           283             2043      47.79             1.025            99.8       0\n",
      "\n",
      "‚úì Performance plot saved: statistics\\performance_20251204_000743.png\n",
      "‚úì Data quality plot saved: statistics\\data_quality_20251204_000743.png\n",
      "‚úì Network graph saved: statistics\\network_seed1_20251204_000743.png\n",
      "‚úì Network graph saved: statistics\\network_seed2_20251204_000743.png\n",
      "‚úì Network graph saved: statistics\\network_seed3_20251204_000743.png\n",
      "‚úì Network graph saved: statistics\\network_seed4_20251204_000743.png\n",
      "‚úì Network graph saved: statistics\\network_seed5_20251204_000743.png\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "Total companies: 42347\n",
      "Total pages crawled: 8099\n",
      "Total duration: 369.0 minutes\n",
      "Average speed: 31.7 pages/min\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SAMPLE RESULTS (First 10 records)\n",
      "======================================================================\n",
      "                                         name                                                                         address      city                phone email website description                                                         source_url   country             crawl_timestamp\n",
      "0                                       basic                                                                                                                                                            https://www.zimbabweyp.com/signup-business/basic  Zimbabwe  2025-12-03T17:58:44.209577\n",
      "2   Caroline Garments P/L t/a Caroline Safety                       Address: 26 Bon Accord Rd, Westondale, Bulawayo, Zimbabwe  Bulawayo  +263 29 22776452003                                   https://www.zimbabweyp.com/company/88429/Caroline_Garments  Zimbabwe  2025-12-03T17:58:48.045811\n",
      "3   Caroline Garments P/L t/a Caroline Safety                       Address: 26 Bon Accord Rd, Westondale, Bulawayo, Zimbabwe  Bulawayo                                                        https://www.zimbabweyp.com/company/88429/Caroline_Garments  Zimbabwe  2025-12-03T17:58:48.046562\n",
      "4              Pest Portal Zimbabwe (Pvt) Ltd  Address: Office 11, 3D Building Strathaven Building, Avondale Harare, Zimbabwe    Harare       07725933442007                               https://www.zimbabweyp.com/company/104158/Pest_Portal_Zimbabwe  Zimbabwe  2025-12-03T17:58:48.050744\n",
      "5              Pest Portal Zimbabwe (Pvt) Ltd  Address: Office 11, 3D Building Strathaven Building, Avondale Harare, Zimbabwe    Harare                                                    https://www.zimbabweyp.com/company/104158/Pest_Portal_Zimbabwe  Zimbabwe  2025-12-03T17:58:48.051524\n",
      "6                       Busy Bee Transcribing                        Address: 13 Ascot Road, Avondale West, Harare , Zimbabwe    Harare      0242-3089692012                              https://www.zimbabweyp.com/company/106087/Busy_Bee_Transcribing  Zimbabwe  2025-12-03T17:58:48.055383\n",
      "7                       Busy Bee Transcribing                        Address: 13 Ascot Road, Avondale West, Harare , Zimbabwe    Harare                                                   https://www.zimbabweyp.com/company/106087/Busy_Bee_Transcribing  Zimbabwe  2025-12-03T17:58:48.055752\n",
      "8                    Athol Evans Hospita Home                        Address: P O Box Cr 70 Cranb Cranborne, Harare, Zimbabwe    Harare             570 8875                            https://www.zimbabweyp.com/company/14492/Athol_Evans_Hospita_Home  Zimbabwe  2025-12-03T17:58:48.057132\n",
      "9                    Athol Evans Hospita Home                        Address: P O Box Cr 70 Cranb Cranborne, Harare, Zimbabwe    Harare                                                 https://www.zimbabweyp.com/company/14492/Athol_Evans_Hospita_Home  Zimbabwe  2025-12-03T17:58:48.058017\n",
      "10                                MEGA MARKET                                          Address: Target Area, Mutare, Zimbabwe    Mutare         263-20-61517                                          https://www.zimbabweyp.com/company/4595/MEGA_MARKET  Zimbabwe  2025-12-03T17:58:48.060322\n",
      "\n",
      "======================================================================\n",
      "FIELD STATISTICS\n",
      "======================================================================\n",
      "Name:         42347 / 42347\n",
      "Address:      39953 / 42347\n",
      "City:         38192 / 42347\n",
      "Phone:        19385 / 42347\n",
      "Email:         1104 / 42347\n",
      "Website:       1634 / 42347\n",
      "Description:   9494 / 42347\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
